{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning for NLP - Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RULES:\n",
    "\n",
    "* Do not create any additional cell\n",
    "\n",
    "* Fill in the blanks\n",
    "\n",
    "* All cells should be runnable (modulo trivial compatibility bugs that we'd fix)\n",
    "\n",
    "* 4 / 20 points will be allocated to the clarity of your code\n",
    "\n",
    "* Efficient code will have a bonus\n",
    "\n",
    "DELIVERABLE:\n",
    "\n",
    "* this notebook\n",
    "* the predictions of the SST test set\n",
    "\n",
    "DO NOT INCLUDE THE DATASETS IN THE DELIVERABLE.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import os\n",
    "import numpy as np\n",
    "import scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
<<<<<<< HEAD
    "PATH_TO_DATA = \"data/\""
=======
    "PATH_TO_DATA = \"../../data/\""
>>>>>>> c4e9ceadf4ee5e2ccdbf1233b43616164c979c88
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1) Monolingual (English) word embeddings "
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 3,
=======
   "execution_count": 5,
>>>>>>> c4e9ceadf4ee5e2ccdbf1233b43616164c979c88
   "metadata": {},
   "outputs": [],
   "source": [
    "class Word2vec():\n",
    "    def __init__(self, fname, nmax=100000):\n",
    "        self.load_wordvec(fname, nmax)\n",
<<<<<<< HEAD
    "        self.word2id = {item:i for i, item in enumerate(self.word2vec.keys())}\n",
=======
    "        self.word2id = dict.fromkeys(self.word2vec.keys())\n",
>>>>>>> c4e9ceadf4ee5e2ccdbf1233b43616164c979c88
    "        self.id2word = {v: k for k, v in self.word2id.items()}\n",
    "        self.embeddings = np.array(self.word2vec.values())\n",
    "    \n",
    "    def load_wordvec(self, fname, nmax):\n",
    "        self.word2vec = {}\n",
    "        with io.open(fname, encoding='utf-8') as f:\n",
    "            next(f)\n",
    "            for i, line in enumerate(f):\n",
    "                word, vec = line.split(' ', 1)\n",
    "                self.word2vec[word] = np.fromstring(vec, sep=' ')\n",
    "                if i == (nmax - 1):\n",
    "                    break\n",
    "        print('Loaded %s pretrained word vectors' % (len(self.word2vec)))\n",
    "\n",
    "    def most_similar(self, w, K=5):\n",
    "        # K most similar words: self.score  -  np.argsort \n",
<<<<<<< HEAD
    "        if type(w)==str:\n",
    "            similarities = [self.score(self.word2vec[w], self.word2vec[self.id2word[i]]) for i in range(len(self.word2id))]\n",
    "        else:\n",
    "            similarities = [self.score(w, self.word2vec[self.id2word[i]]) for i in range(len(self.word2id))]\n",
    "        closest_k = np.argsort(similarities)[::-1][:K]\n",
    "        results = [self.id2word[i] for i in closest_k]\n",
    "        return results\n",
    "\n",
    "    def score(self, w1, w2):\n",
    "        # cosine similarity: np.dot  -  np.linalg.norm\n",
    "        if type(w1)==str:\n",
    "            w2v1 = self.word2vec[w1]\n",
    "            w2v2 = self.word2vec[w2]\n",
    "        else:\n",
    "            w2v1 = w1\n",
    "            w2v2 = w2\n",
    "        \n",
    "        score = np.dot(w2v1, w2v2)  / (np.linalg.norm(w2v1)*np.linalg.norm(w2v2))\n",
    "        return score"
=======
    "        similiarities = [self.score(w, self.word2vec[self.id2word[i]]) for i in range(len(self.word2id))]\n",
    "        closest_k = np.argsort(similiarities)[::-1][:K]\n",
    "        results = [self.id2word[i] for i in closest_k]\n",
    "        return\n",
    "\n",
    "    def score(self, w1, w2):\n",
    "        # cosine similarity: np.dot  -  np.linalg.norm\n",
    "        w2v1 = self.word2vec[w1]\n",
    "        w2v2 = self.word2vec[w2]\n",
    "        \n",
    "        score = np.dot(w2v1, w2v2)  / (np.linalg.norm(w2v1)*np.linalg.norm(w2v1))\n",
    "        return score\n"
>>>>>>> c4e9ceadf4ee5e2ccdbf1233b43616164c979c88
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 100000 pretrained word vectors\n",
      "cat dog 0.671683666279249\n",
      "dog pet 0.6842064029669219\n",
      "dogs cats 0.7074389328052404\n",
      "paris france 0.7775108541288563\n",
      "germany berlin 0.7420295235998394\n",
      "['cat', 'cats', 'kitty', 'kitten', 'feline']\n",
      "['dog', 'dogs', 'puppy', 'Dog', 'doggie']\n",
      "['dogs', 'dog', 'pooches', 'Dogs', 'doggies']\n",
      "['paris', 'france', 'Paris', 'london', 'berlin']\n",
      "['germany', 'austria', 'europe', 'german', 'berlin']\n"
=======
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../../data/crawl-300d-200k.vec'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-281c4975d070>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mw2v\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mWord2vec\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mPATH_TO_DATA\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'crawl-300d-200k.vec'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnmax\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m25000\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m# You will be evaluated on the output of the following:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mw1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw2\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'cat'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'dog'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'dogs'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'paris'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'germany'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m'dog'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'pet'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'cats'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'france'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'berlin'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw2v\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-5-c6bc745eca2f>\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, fname, nmax)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mWord2vec\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnmax\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100000\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_wordvec\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnmax\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mword2id\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfromkeys\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mword2vec\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mid2word\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[0mv\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mk\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mv\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mword2id\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-5-c6bc745eca2f>\u001b[0m in \u001b[0;36mload_wordvec\u001b[1;34m(self, fname, nmax)\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mload_wordvec\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnmax\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mword2vec\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m         \u001b[1;32mwith\u001b[0m \u001b[0mio\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'utf-8'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m             \u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mline\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../../data/crawl-300d-200k.vec'"
>>>>>>> c4e9ceadf4ee5e2ccdbf1233b43616164c979c88
     ]
    }
   ],
   "source": [
<<<<<<< HEAD
    "# need to change load number from 25,000 to 100,000 to have \"Paris\" for ex\n",
    "w2v = Word2vec(os.path.join(PATH_TO_DATA, 'crawl-300d-200k.vec'), nmax=100000)\n",
=======
    "w2v = Word2vec(os.path.join(PATH_TO_DATA, 'crawl-300d-200k.vec'), nmax=25000)\n",
>>>>>>> c4e9ceadf4ee5e2ccdbf1233b43616164c979c88
    "\n",
    "# You will be evaluated on the output of the following:\n",
    "for w1, w2 in zip(('cat', 'dog', 'dogs', 'paris', 'germany'), ('dog', 'pet', 'cats', 'france', 'berlin')):\n",
    "    print(w1, w2, w2v.score(w1, w2))\n",
    "for w1 in ['cat', 'dog', 'dogs', 'paris', 'germany']:\n",
    "    print(w2v.most_similar(w1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BoV():\n",
    "    def __init__(self, w2v):\n",
    "        self.w2v = w2v\n",
    "    \n",
    "    def encode(self, sentences, idf=False):\n",
    "        # takes a list of sentences, outputs a numpy array of sentence embeddings\n",
    "        # see TP1 for help\n",
    "        sentemb = []\n",
<<<<<<< HEAD
    "        for sent in sentences:\n",
    "            if idf is False:\n",
    "                # mean of word vectors\n",
    "                sentemb.append(np.mean([self.w2v.word2vec[w] for w in sent if w in self.w2v.word2vec], axis=0))\n",
    "            else:\n",
    "                # idf-weighted mean of word vectors\n",
    "                sentemb.append(np.mean([self.w2v.word2vec[w]*idf[w] for w in sent if w in self.w2v.word2vec], axis=0))\n",
=======
    "        return\n",
    "        for sent in sentences:\n",
    "            if idf is False:\n",
    "                # mean of word vectors\n",
    "                sentemb += [np.mean([self.w2v.word2vec[w] for w in sent if w in self.w2v.word2vec], axis=0)]\n",
    "            else:\n",
    "                # idf-weighted mean of word vectors\n",
    "                sentemb += [np.mean([self.w2v.word2vec[w]*idf[w] for w in sent if w in self.w2v.word2vec], axis=0)]\n",
>>>>>>> c4e9ceadf4ee5e2ccdbf1233b43616164c979c88
    "        return np.vstack(sentemb)\n",
    "\n",
    "    def most_similar(self, s, sentences, idf=False, K=5):\n",
    "        # get most similar sentences and **print** them\n",
    "        keys = self.encode(sentences, idf)\n",
    "        query = self.encode([s], idf)\n",
    "        \n",
    "        scores = np.dot(keys, query.T)\n",
<<<<<<< HEAD
    "        scores /= np.reshape(np.linalg.norm(keys, axis=1), (-1,1))\n",
    "        closest_sentences = np.argsort(scores[:,0])\n",
    "        closest_sentences = [sentences[i] for i in closest_sentences]\n",
=======
    "        scores = scores / np.linalg.norm(keys, axis=1).reshape(-1,1))\n",
    "        closest_sentences = np.argsort(scores[:,0])      \n",
>>>>>>> c4e9ceadf4ee5e2ccdbf1233b43616164c979c88
    "        print(closest_sentences[::-1][:K])\n",
    "\n",
    "    def score(self, s1, s2, idf=False):\n",
    "        # cosine similarity: use   np.dot  and  np.linalg.norm\n",
    "        enc1 = self.encode([s1], idf)\n",
    "        enc2 = self.encode([s2], idf)\n",
    "        \n",
<<<<<<< HEAD
    "        res = np.dot(enc1,enc2.T) \\\n",
    "            /(np.linalg.norm(enc1) * np.linalg.norm(enc2)) \n",
    "        \n",
    "        return print(res[0,0])\n",
=======
    "        score = np.dot(enc1,enc2.T) /(np.linalg.norm(enc1) * np.linalg.norm(enc2)) \n",
    "        return score\n",
>>>>>>> c4e9ceadf4ee5e2ccdbf1233b43616164c979c88
    "    \n",
    "    def build_idf(self, sentences):\n",
    "        # build the idf dictionary: associate each word to its idf value\n",
    "        idf = {}        \n",
    "        for sent in sentences:\n",
    "            for w in set(sent):\n",
    "                idf[w] = idf.get(w, 0) + 1\n",
    "        \n",
<<<<<<< HEAD
    "        idf = {w:max(1, np.log10(len(sentences) / v)) for (w,v) in idf.items()}\n",
    "        return idf\n"
=======
    "        idf = {word:max(1, np.log10(len(sentences) / val)) for (word, val) in idf.items()}\n",
    "        return idf\n",
    "        # for sent in sentences:\n",
    "        #     for w in set(sent):\n",
    "        #         idf[w] = idf.get(w, 0) + 1\n",
    "        \n",
    "        # max(1, np.log10(len(sentences) / (idf[word])))\n"
>>>>>>> c4e9ceadf4ee5e2ccdbf1233b43616164c979c88
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
      "Loaded 10000 pretrained word vectors\n",
      "[['1', 'smiling', 'african', 'american', 'boy', '.'], ['girl', 'smiling', 'on', 'roller', 'coaster', '.'], ['a', 'boy', 'smiles', 'underwater', '.'], ['two', 'girlfriends', 'smiling', '.'], ['a', 'smiling', 'child', 'swims', '.']]\n",
      "0.5936035444177407\n",
      "[['1', 'smiling', 'african', 'american', 'boy', '.'], ['5', 'women', 'and', '1', 'man', 'are', 'smiling', 'for', 'the', 'camera', '.'], ['2', 'guys', 'facing', 'away', 'from', 'camera', ',', '1', 'girl', 'smiling', 'at', 'camera', 'with', 'blue', 'shirt', ',', '1', 'guy', 'with', 'a', 'beverage', 'with', 'a', 'jacket', 'on', '.'], ['two', 'girlfriends', 'smiling', '.'], ['1', 'man', 'singing', 'and', '1', 'man', 'playing', 'a', 'saxophone', 'in', 'a', 'concert', '.']]\n",
      "0.5004864584233165\n"
=======
      "Loaded 5000 pretrained word vectors\n"
>>>>>>> c4e9ceadf4ee5e2ccdbf1233b43616164c979c88
     ]
    }
   ],
   "source": [
<<<<<<< HEAD
    "w2v = Word2vec(os.path.join(PATH_TO_DATA, 'crawl-300d-200k.vec'), nmax=10000)\n",
=======
    "w2v = Word2vec(os.path.join(PATH_TO_DATA, 'crawl-300d-200k.vec'), nmax=5000)\n",
>>>>>>> c4e9ceadf4ee5e2ccdbf1233b43616164c979c88
    "s2v = BoV(w2v)\n",
    "\n",
    "# Load sentences in \"PATH_TO_DATA/sentences.txt\"\n",
    "file = open(os.path.join(PATH_TO_DATA, 'sentences.txt'), \"r\")\n",
    "sentences = [line.split() for line in file]\n",
    "\n",
    "# Build idf scores for each word\n",
    "idf = s2v.build_idf(sentences)\n",
    "\n",
    "# You will be evaluated on the output of the following:\n",
    "s2v.most_similar('' if not sentences else sentences[10], sentences)  # BoV-mean\n",
    "s2v.score('' if not sentences else sentences[7], '' if not sentences else sentences[13])\n",
    "\n",
    "\n",
    "\n",
    "s2v.most_similar('' if not sentences else sentences[10], sentences, idf)  # BoV-idf\n",
    "s2v.score('' if not sentences else sentences[7], '' if not sentences else sentences[13], idf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2) Multilingual (English-French) word embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's consider a bilingual dictionary of size V_a (e.g French-English).\n",
    "\n",
    "Let's define **X** and **Y** the **French** and **English** matrices.\n",
    "\n",
    "They contain the embeddings associated to the words in the bilingual dictionary.\n",
    "\n",
    "We want to find a **mapping W** that will project the source word space (e.g French) to the target word space (e.g English).\n",
    "\n",
    "Procrustes : **W\\* = argmin || W.X - Y ||  s.t  W^T.W = Id**\n",
    "has a closed form solution:\n",
    "**W = U.V^T  where  U.Sig.V^T = SVD(Y.X^T)**\n",
    "\n",
    "In what follows, you are asked to: "
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 50000 pretrained word vectors\n",
      "Loaded 50000 pretrained word vectors\n"
     ]
    }
   ],
=======
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
>>>>>>> c4e9ceadf4ee5e2ccdbf1233b43616164c979c88
   "source": [
    "# 1 - Download and load 50k first vectors of\n",
    "#     https://s3-us-west-1.amazonaws.com/fasttext-vectors/wiki.en.vec\n",
    "#     https://s3-us-west-1.amazonaws.com/fasttext-vectors/wiki.fr.vec\n",
    "\n",
    "# TYPE CODE HERE\n",
    "w2v_en = Word2vec(os.path.join(PATH_TO_DATA, 'wiki.en.vec'), nmax=50000)\n",
    "w2v_fr = Word2vec(os.path.join(PATH_TO_DATA, 'wiki.fr.vec'), nmax=50000)\n"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 8,
   "metadata": {},
=======
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
>>>>>>> c4e9ceadf4ee5e2ccdbf1233b43616164c979c88
   "outputs": [],
   "source": [
    "# 2 - Get words that appear in both vocabs (= identical character strings)\n",
    "#     Use it to create the matrix X and Y (of aligned embeddings for these words)\n",
    "\n",
    "# TYPE CODE HERE\n",
    "dictX = {w: w2v_fr.word2vec[w] for w in w2v_en.word2vec if w in w2v_fr.word2vec}\n",
<<<<<<< HEAD
    "dictY = {w: w2v_en.word2vec[w] for w in dictX}\n",
=======
    "dictY = {w: w2v_en.word2vec[w] for w in dict_X}\n",
>>>>>>> c4e9ceadf4ee5e2ccdbf1233b43616164c979c88
    "\n",
    "X = np.array([dictX[w] for w in dictX]).T\n",
    "Y = np.array([dictY[w] for w in dictX]).T"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 9,
   "metadata": {},
=======
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
>>>>>>> c4e9ceadf4ee5e2ccdbf1233b43616164c979c88
   "outputs": [],
   "source": [
    "# 3 - Solve the Procrustes using the scipy package and: scipy.linalg.svd() and get the optimal W\n",
    "#     Now W*French_vector is in the same space as English_vector\n",
    "\n",
    "# TYPE CODE HERE\n",
    "U, S, V = np.linalg.svd(np.dot(Y, X.T), full_matrices=False)\n",
    "W = np.dot(U, V)\n",
    "W_inv = np.linalg.inv(W)"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From French to English\n",
      "Most similar words for voiture are :  ['car', 'cars', 'limousine', 'suv', 'roadster']\n",
      "Most similar words for facebook are :  ['facebook', 'twitter', 'myspace', 'reddit', 'tumblr']\n",
      "Most similar words for nettoyé are :  ['cleaned', 'vandalized', 'cleaning', 'removing', 'vandalised']\n",
      "Most similar words for jacques are :  ['jacques', 'françois', 'jean', 'henri', 'philippe']\n",
      "Most similar words for avant are :  ['before', 'shortly', 'être', 'avant', 'immediately']\n",
      "Most similar words for robuste are :  ['robust', 'sturdy', 'agile', 'resilient', 'adaptable']\n",
      "\n",
      "\n",
      "From English to French\n",
      "Most similar words for manifold are :  ['topologique', 'ℝ', 'euclidien', 'isomorphisme', 'abélien']\n",
      "Most similar words for actually are :  ['would', 'should', 'could', 'that', 'really']\n",
      "Most similar words for fake are :  ['fake', 'arnaque', 'fuck', 'canulars', 'canular']\n",
      "Most similar words for learning are :  ['learning', 'education', 'apprentissage', 'apprentissages', 'educational']\n",
      "Most similar words for love are :  ['love', 'amour', 'lover', 'lovers', 'beautiful']\n"
     ]
    }
   ],
=======
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
>>>>>>> c4e9ceadf4ee5e2ccdbf1233b43616164c979c88
   "source": [
    "# 4 - After alignment with W, give examples of English nearest neighbors of some French words (and vice versa)\n",
    "#     You will be evaluated on that part and the code above\n",
    "\n",
    "# TYPE CODE HERE\n",
<<<<<<< HEAD
    "print('From French to English')\n",
    "for w in ['voiture', 'facebook', 'nettoyé', 'jacques', 'avant','robuste']:\n",
    "    emb = np.dot(W, w2v_fr.word2vec[w])\n",
    "    print('Most similar words for %s are : ' %  w, str(w2v_en.most_similar(emb)))\n",
    "    \n",
    "print('\\n')\n",
    "\n",
    "print('From English to French')\n",
    "for w in ['manifold', 'actually', 'fake', 'learning','love']:\n",
    "    emb = np.dot(W_inv, w2v_en.word2vec[w])\n",
    "    print('Most similar words for %s are : ' % w, str(w2v_fr.most_similar(emb)) )"
=======
    "print('French to English')\n",
    "#for w in ['voiture', 'facebook', 'nettoyé', 'jacques', 'avant','robuste']:\n",
    "    emb = np.dot(W, w2v_fr.word2vec[w])\n",
    "    print('Most similar words for %s are : ' %w + str(w2v_en.most_similar(embedding)))\n",
    "    \n",
    "print('\\n')\n",
    "\n",
    "print('English to French')\n",
    "#for w in ['manifold', 'actually', 'fake', 'learning','love']:\n",
    "    emb = np.dot(W_inv, w2v_en.word2vec[w])\n",
    "    print('Most similar words for %s are : ' %w + str(w2v_fr.most_similar(embedding)) )"
>>>>>>> c4e9ceadf4ee5e2ccdbf1233b43616164c979c88
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to dive deeper on this subject: https://github.com/facebookresearch/MUSE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3) Sentence classification with BoV and scikit-learn"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 11,
   "metadata": {},
=======
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
>>>>>>> c4e9ceadf4ee5e2ccdbf1233b43616164c979c88
   "outputs": [],
   "source": [
    "# 1 - Load train/dev/test of Stanford Sentiment TreeBank (SST)\n",
    "#     (https://nlp.stanford.edu/~socherr/EMNLP2013_RNTN.pdf)\n",
    "\n",
    "# TYPE CODE HERE\n",
<<<<<<< HEAD
    "train_data = []\n",
    "labels_train = []\n",
    "file = open(os.path.join(PATH_TO_DATA, 'SST/stsa.fine.train'), \"r\")\n",
    "for line in file:\n",
    "    train_data.append(line.split()[1:]) \n",
    "    labels_train.append(line.split()[0])\n",
    "labels_train = [int(labels_train[i]) for i in range(len(labels_train))]\n",
    "\n",
    "# validation\n",
    "valid_data = []\n",
    "labels_valid = []\n",
    "file = open(os.path.join(PATH_TO_DATA, 'SST/stsa.fine.dev'), \"r\")\n",
    "for line in file:\n",
    "    valid_data.append(line.split()[1:])\n",
    "    labels_valid.append(line.split()[0])\n",
    "labels_valid = [int(labels_valid[i]) for i in range(len(labels_valid))]\n",
    "\n",
    "# test\n",
    "test_data = []\n",
    "file = open(os.path.join(PATH_TO_DATA, 'SST/stsa.fine.test.X'), \"r\")\n",
    "for line in file:\n",
    "    test_data.append(line.split())"
=======
    "train = []\n",
    "labels_train = []\n",
    "file = open(os.path.join(PATH_TO_DATA, 'SST/stsa.fine.train'), \"r\")\n",
    "for line in file:\n",
    "    train.append(line.split()[1:]) \n",
    "    labels_train.append(line.split()[0])\n",
    "labels_train = [int(labels_train[i]) for i in range(len(labels_train))] #Converting strings of integer into integers\n",
    "\n",
    "#DEV SET\n",
    "dev = []\n",
    "labels_dev = []\n",
    "file = open(os.path.join(PATH_TO_DATA, 'SST/stsa.fine.dev'), \"r\")\n",
    "for line in file:\n",
    "    dev.append(line.split()[1:])\n",
    "    labels_dev.append(line.split()[0])\n",
    "labels_dev = [int(labels_dev[i]) for i in range(len(labels_dev))]\n",
    "\n",
    "#TEST SET\n",
    "test = []\n",
    "file = open(os.path.join(PATH_TO_DATA, 'SST/stsa.fine.test.X'), \"r\")\n",
    "for line in file:\n",
    "    test.append(line.split())"
>>>>>>> c4e9ceadf4ee5e2ccdbf1233b43616164c979c88
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 10000 pretrained word vectors\n"
     ]
    }
   ],
   "source": [
    "# 2 - Encode sentences with the BoV model above\n",
    "\n",
    "w2v = Word2vec(os.path.join(PATH_TO_DATA, 'crawl-300d-200k.vec'), nmax=10000)\n",
    "s2v = BoV(w2v)\n",
    "\n",
    "idf = s2v.build_idf(train_data + valid_data + test_data)\n",
    "\n",
    "encoded_train_data = s2v.encode(train_data, False)\n",
    "encoded_valid_data = s2v.encode(valid_data, False)\n",
    "encoded_test_data = s2v.encode(test_data, False)\n",
    "\n",
    "encoded_train_data_weighted = s2v.encode(train_data, idf)\n",
    "encoded_valid_data_weighted = s2v.encode(valid_data, idf)\n",
    "encoded_test_data_weighted = s2v.encode(test_data, idf)"
=======
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 2 - Encode sentences with the BoV model above\n",
    "\n",
    "# TYPE CODE HERE"
>>>>>>> c4e9ceadf4ee5e2ccdbf1233b43616164c979c88
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train score: 0.471 \n",
      "Valid score: 0.400\n",
      "Train score (weighted): 0.450 \n",
      "Valid score (weighted): 0.399\n"
     ]
    }
   ],
=======
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
>>>>>>> c4e9ceadf4ee5e2ccdbf1233b43616164c979c88
   "source": [
    "# 3 - Learn Logistic Regression on top of sentence embeddings using scikit-learn\n",
    "#     (consider tuning the L2 regularization on the dev set)\n",
    "\n",
<<<<<<< HEAD
    "from sklearn import preprocessing\n",
    "from sklearn import linear_model\n",
    "\n",
    "scaler1 = preprocessing.StandardScaler().fit(encoded_train_data)\n",
    "normalized_encoded_train_data = scaler1.transform(encoded_train_data)\n",
    "normalized_encoded_valid_data = scaler1.transform(encoded_valid_data)\n",
    "normalized_encoded_test_data = scaler1.transform(encoded_test_data)\n",
    "\n",
    "scaler2 = preprocessing.StandardScaler().fit(encoded_train_data_weighted)\n",
    "normalized_encoded_train_data_weighted = scaler2.transform(encoded_train_data_weighted)\n",
    "normalized_encoded_valid_data_weighted = scaler2.transform(encoded_valid_data_weighted)\n",
    "normalized_encoded_test_data_weighted = scaler2.transform(encoded_test_data_weighted)\n",
    "\n",
    "logreg_model1 = linear_model.LogisticRegression(C=1,penalty='l2',tol=1e-6)\n",
    "logreg_model1.fit(normalized_encoded_train_data, labels_train)\n",
    "\n",
    "logreg_model2 = linear_model.LogisticRegression(C=1,penalty='l2',tol=1e-6)\n",
    "logreg_model2.fit(normalized_encoded_train_data_weighted, labels_train)\n",
    "\n",
    "print('Train score: %.3f ' % logreg_model1.score(normalized_encoded_train_data, labels_train))\n",
    "print('Valid score: %.3f' % logreg_model1.score(normalized_encoded_valid_data,labels_valid))\n",
    "\n",
    "print('Train score (weighted): %.3f ' % logreg_model2.score(normalized_encoded_train_data, labels_train))\n",
    "print('Valid score (weighted): %.3f' % logreg_model2.score(normalized_encoded_valid_data,labels_valid))"
=======
    "# TYPE CODE HERE"
>>>>>>> c4e9ceadf4ee5e2ccdbf1233b43616164c979c88
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 14,
   "metadata": {},
=======
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
>>>>>>> c4e9ceadf4ee5e2ccdbf1233b43616164c979c88
   "outputs": [],
   "source": [
    "# 4 - Produce 2210 predictions for the test set (in the same order). One line = one prediction (=0,1,2,3,4).\n",
    "#     Attach the output file \"logreg_bov_y_test_sst.txt\" to your deliverable.\n",
    "#     You will be evaluated on the results of the test set.\n",
    "\n",
<<<<<<< HEAD
    "\n",
    "prediction_test=logreg_model1.predict(normalized_encoded_test_data)\n",
    "\n",
    "with open('./logreg_bov_y_test_sst.txt', 'w') as f1:\n",
    "    for k in prediction_test:\n",
    "        f1.write(str(k) + os.linesep)"
=======
    "# TYPE CODE HERE"
>>>>>>> c4e9ceadf4ee5e2ccdbf1233b43616164c979c88
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\tvalid_0's multi_logloss: 1.60681\n",
      "Training until validation scores don't improve for 20 rounds.\n",
      "[2]\tvalid_0's multi_logloss: 1.60428\n",
      "[3]\tvalid_0's multi_logloss: 1.60185\n",
      "[4]\tvalid_0's multi_logloss: 1.59953\n",
      "[5]\tvalid_0's multi_logloss: 1.59729\n",
      "[6]\tvalid_0's multi_logloss: 1.5951\n",
      "[7]\tvalid_0's multi_logloss: 1.59304\n",
      "[8]\tvalid_0's multi_logloss: 1.59095\n",
      "[9]\tvalid_0's multi_logloss: 1.58899\n",
      "[10]\tvalid_0's multi_logloss: 1.58703\n",
      "[11]\tvalid_0's multi_logloss: 1.58518\n",
      "[12]\tvalid_0's multi_logloss: 1.58338\n",
      "[13]\tvalid_0's multi_logloss: 1.58173\n",
      "[14]\tvalid_0's multi_logloss: 1.58003\n",
      "[15]\tvalid_0's multi_logloss: 1.57846\n",
      "[16]\tvalid_0's multi_logloss: 1.57683\n",
      "[17]\tvalid_0's multi_logloss: 1.57529\n",
      "[18]\tvalid_0's multi_logloss: 1.57382\n",
      "[19]\tvalid_0's multi_logloss: 1.57232\n",
      "[20]\tvalid_0's multi_logloss: 1.57091\n",
      "[21]\tvalid_0's multi_logloss: 1.5695\n",
      "[22]\tvalid_0's multi_logloss: 1.5681\n",
      "[23]\tvalid_0's multi_logloss: 1.56683\n",
      "[24]\tvalid_0's multi_logloss: 1.56551\n",
      "[25]\tvalid_0's multi_logloss: 1.56431\n",
      "[26]\tvalid_0's multi_logloss: 1.56313\n",
      "[27]\tvalid_0's multi_logloss: 1.56195\n",
      "[28]\tvalid_0's multi_logloss: 1.56076\n",
      "[29]\tvalid_0's multi_logloss: 1.55969\n",
      "[30]\tvalid_0's multi_logloss: 1.55861\n",
      "[31]\tvalid_0's multi_logloss: 1.55753\n",
      "[32]\tvalid_0's multi_logloss: 1.55645\n",
      "[33]\tvalid_0's multi_logloss: 1.55542\n",
      "[34]\tvalid_0's multi_logloss: 1.55425\n",
      "[35]\tvalid_0's multi_logloss: 1.55323\n",
      "[36]\tvalid_0's multi_logloss: 1.55229\n",
      "[37]\tvalid_0's multi_logloss: 1.55126\n",
      "[38]\tvalid_0's multi_logloss: 1.55035\n",
      "[39]\tvalid_0's multi_logloss: 1.5494\n",
      "[40]\tvalid_0's multi_logloss: 1.54846\n",
      "[41]\tvalid_0's multi_logloss: 1.54763\n",
      "[42]\tvalid_0's multi_logloss: 1.54678\n",
      "[43]\tvalid_0's multi_logloss: 1.54588\n",
      "[44]\tvalid_0's multi_logloss: 1.54498\n",
      "[45]\tvalid_0's multi_logloss: 1.5441\n",
      "[46]\tvalid_0's multi_logloss: 1.54327\n",
      "[47]\tvalid_0's multi_logloss: 1.54263\n",
      "[48]\tvalid_0's multi_logloss: 1.54183\n",
      "[49]\tvalid_0's multi_logloss: 1.54103\n",
      "[50]\tvalid_0's multi_logloss: 1.54031\n",
      "[51]\tvalid_0's multi_logloss: 1.53956\n",
      "[52]\tvalid_0's multi_logloss: 1.53883\n",
      "[53]\tvalid_0's multi_logloss: 1.53813\n",
      "[54]\tvalid_0's multi_logloss: 1.53742\n",
      "[55]\tvalid_0's multi_logloss: 1.53668\n",
      "[56]\tvalid_0's multi_logloss: 1.53606\n",
      "[57]\tvalid_0's multi_logloss: 1.53529\n",
      "[58]\tvalid_0's multi_logloss: 1.53457\n",
      "[59]\tvalid_0's multi_logloss: 1.53393\n",
      "[60]\tvalid_0's multi_logloss: 1.53329\n",
      "[61]\tvalid_0's multi_logloss: 1.53263\n",
      "[62]\tvalid_0's multi_logloss: 1.53196\n",
      "[63]\tvalid_0's multi_logloss: 1.53139\n",
      "[64]\tvalid_0's multi_logloss: 1.53081\n",
      "[65]\tvalid_0's multi_logloss: 1.5302\n",
      "[66]\tvalid_0's multi_logloss: 1.52953\n",
      "[67]\tvalid_0's multi_logloss: 1.52902\n",
      "[68]\tvalid_0's multi_logloss: 1.52845\n",
      "[69]\tvalid_0's multi_logloss: 1.52788\n",
      "[70]\tvalid_0's multi_logloss: 1.52728\n",
      "[71]\tvalid_0's multi_logloss: 1.5268\n",
      "[72]\tvalid_0's multi_logloss: 1.52628\n",
      "[73]\tvalid_0's multi_logloss: 1.52576\n",
      "[74]\tvalid_0's multi_logloss: 1.5252\n",
      "[75]\tvalid_0's multi_logloss: 1.52468\n",
      "[76]\tvalid_0's multi_logloss: 1.52414\n",
      "[77]\tvalid_0's multi_logloss: 1.52359\n",
      "[78]\tvalid_0's multi_logloss: 1.52308\n",
      "[79]\tvalid_0's multi_logloss: 1.52255\n",
      "[80]\tvalid_0's multi_logloss: 1.52212\n",
      "[81]\tvalid_0's multi_logloss: 1.5216\n",
      "[82]\tvalid_0's multi_logloss: 1.52115\n",
      "[83]\tvalid_0's multi_logloss: 1.52065\n",
      "[84]\tvalid_0's multi_logloss: 1.52022\n",
      "[85]\tvalid_0's multi_logloss: 1.51973\n",
      "[86]\tvalid_0's multi_logloss: 1.51931\n",
      "[87]\tvalid_0's multi_logloss: 1.51892\n",
      "[88]\tvalid_0's multi_logloss: 1.51847\n",
      "[89]\tvalid_0's multi_logloss: 1.51802\n",
      "[90]\tvalid_0's multi_logloss: 1.51752\n",
      "[91]\tvalid_0's multi_logloss: 1.5172\n",
      "[92]\tvalid_0's multi_logloss: 1.51677\n",
      "[93]\tvalid_0's multi_logloss: 1.51631\n",
      "[94]\tvalid_0's multi_logloss: 1.51591\n",
      "[95]\tvalid_0's multi_logloss: 1.51549\n",
      "[96]\tvalid_0's multi_logloss: 1.51502\n",
      "[97]\tvalid_0's multi_logloss: 1.51458\n",
      "[98]\tvalid_0's multi_logloss: 1.51422\n",
      "[99]\tvalid_0's multi_logloss: 1.51384\n",
      "[100]\tvalid_0's multi_logloss: 1.51339\n",
      "[101]\tvalid_0's multi_logloss: 1.51301\n",
      "[102]\tvalid_0's multi_logloss: 1.51261\n",
      "[103]\tvalid_0's multi_logloss: 1.51215\n",
      "[104]\tvalid_0's multi_logloss: 1.51177\n",
      "[105]\tvalid_0's multi_logloss: 1.51142\n",
      "[106]\tvalid_0's multi_logloss: 1.51098\n",
      "[107]\tvalid_0's multi_logloss: 1.51056\n",
      "[108]\tvalid_0's multi_logloss: 1.51013\n",
      "[109]\tvalid_0's multi_logloss: 1.50975\n",
      "[110]\tvalid_0's multi_logloss: 1.50937\n",
      "[111]\tvalid_0's multi_logloss: 1.50889\n",
      "[112]\tvalid_0's multi_logloss: 1.50844\n",
      "[113]\tvalid_0's multi_logloss: 1.50817\n",
      "[114]\tvalid_0's multi_logloss: 1.5078\n",
      "[115]\tvalid_0's multi_logloss: 1.50732\n",
      "[116]\tvalid_0's multi_logloss: 1.50696\n",
      "[117]\tvalid_0's multi_logloss: 1.50657\n",
      "[118]\tvalid_0's multi_logloss: 1.50627\n",
      "[119]\tvalid_0's multi_logloss: 1.50592\n",
      "[120]\tvalid_0's multi_logloss: 1.50549\n",
      "[121]\tvalid_0's multi_logloss: 1.50513\n",
      "[122]\tvalid_0's multi_logloss: 1.50483\n",
      "[123]\tvalid_0's multi_logloss: 1.50454\n",
      "[124]\tvalid_0's multi_logloss: 1.50414\n",
      "[125]\tvalid_0's multi_logloss: 1.50386\n",
      "[126]\tvalid_0's multi_logloss: 1.50348\n",
      "[127]\tvalid_0's multi_logloss: 1.50322\n",
      "[128]\tvalid_0's multi_logloss: 1.50297\n",
      "[129]\tvalid_0's multi_logloss: 1.50251\n",
      "[130]\tvalid_0's multi_logloss: 1.50221\n",
      "[131]\tvalid_0's multi_logloss: 1.50195\n",
      "[132]\tvalid_0's multi_logloss: 1.50165\n",
      "[133]\tvalid_0's multi_logloss: 1.50137\n",
      "[134]\tvalid_0's multi_logloss: 1.50091\n",
      "[135]\tvalid_0's multi_logloss: 1.50067\n",
      "[136]\tvalid_0's multi_logloss: 1.50032\n",
      "[137]\tvalid_0's multi_logloss: 1.50002\n",
      "[138]\tvalid_0's multi_logloss: 1.49967\n",
      "[139]\tvalid_0's multi_logloss: 1.49937\n",
      "[140]\tvalid_0's multi_logloss: 1.49905\n",
      "[141]\tvalid_0's multi_logloss: 1.49878\n",
      "[142]\tvalid_0's multi_logloss: 1.49843\n",
      "[143]\tvalid_0's multi_logloss: 1.49811\n",
      "[144]\tvalid_0's multi_logloss: 1.49784\n",
      "[145]\tvalid_0's multi_logloss: 1.49758\n",
      "[146]\tvalid_0's multi_logloss: 1.49738\n",
      "[147]\tvalid_0's multi_logloss: 1.49694\n",
      "[148]\tvalid_0's multi_logloss: 1.49674\n",
      "[149]\tvalid_0's multi_logloss: 1.49643\n",
      "[150]\tvalid_0's multi_logloss: 1.49616\n",
      "[151]\tvalid_0's multi_logloss: 1.49587\n",
      "[152]\tvalid_0's multi_logloss: 1.49559\n",
      "[153]\tvalid_0's multi_logloss: 1.49518\n",
      "[154]\tvalid_0's multi_logloss: 1.49495\n",
      "[155]\tvalid_0's multi_logloss: 1.49471\n",
      "[156]\tvalid_0's multi_logloss: 1.49447\n",
      "[157]\tvalid_0's multi_logloss: 1.49434\n",
      "[158]\tvalid_0's multi_logloss: 1.49402\n",
      "[159]\tvalid_0's multi_logloss: 1.49372\n",
      "[160]\tvalid_0's multi_logloss: 1.49349\n",
      "[161]\tvalid_0's multi_logloss: 1.49314\n",
      "[162]\tvalid_0's multi_logloss: 1.49296\n",
      "[163]\tvalid_0's multi_logloss: 1.49265\n",
      "[164]\tvalid_0's multi_logloss: 1.49241\n",
      "[165]\tvalid_0's multi_logloss: 1.4921\n",
      "[166]\tvalid_0's multi_logloss: 1.49192\n",
      "[167]\tvalid_0's multi_logloss: 1.49172\n",
      "[168]\tvalid_0's multi_logloss: 1.49152\n",
      "[169]\tvalid_0's multi_logloss: 1.49127\n",
      "[170]\tvalid_0's multi_logloss: 1.49095\n",
      "[171]\tvalid_0's multi_logloss: 1.49068\n",
      "[172]\tvalid_0's multi_logloss: 1.49043\n",
      "[173]\tvalid_0's multi_logloss: 1.49019\n",
      "[174]\tvalid_0's multi_logloss: 1.48995\n",
      "[175]\tvalid_0's multi_logloss: 1.48969\n",
      "[176]\tvalid_0's multi_logloss: 1.48946\n",
      "[177]\tvalid_0's multi_logloss: 1.48935\n",
      "[178]\tvalid_0's multi_logloss: 1.4891\n",
      "[179]\tvalid_0's multi_logloss: 1.48877\n",
      "[180]\tvalid_0's multi_logloss: 1.48853\n",
      "[181]\tvalid_0's multi_logloss: 1.48833\n",
      "[182]\tvalid_0's multi_logloss: 1.48813\n",
      "[183]\tvalid_0's multi_logloss: 1.48785\n",
      "[184]\tvalid_0's multi_logloss: 1.48775\n",
      "[185]\tvalid_0's multi_logloss: 1.48747\n",
      "[186]\tvalid_0's multi_logloss: 1.48723\n",
      "[187]\tvalid_0's multi_logloss: 1.48702\n",
      "[188]\tvalid_0's multi_logloss: 1.48676\n",
      "[189]\tvalid_0's multi_logloss: 1.4865\n",
      "[190]\tvalid_0's multi_logloss: 1.48623\n",
      "[191]\tvalid_0's multi_logloss: 1.48598\n",
      "[192]\tvalid_0's multi_logloss: 1.48572\n",
      "[193]\tvalid_0's multi_logloss: 1.48551\n",
      "[194]\tvalid_0's multi_logloss: 1.48532\n",
      "[195]\tvalid_0's multi_logloss: 1.48511\n",
      "[196]\tvalid_0's multi_logloss: 1.48484\n",
      "[197]\tvalid_0's multi_logloss: 1.48475\n",
      "[198]\tvalid_0's multi_logloss: 1.48458\n",
      "[199]\tvalid_0's multi_logloss: 1.48434\n",
      "[200]\tvalid_0's multi_logloss: 1.48409\n",
      "[201]\tvalid_0's multi_logloss: 1.48388\n",
      "[202]\tvalid_0's multi_logloss: 1.48368\n",
      "[203]\tvalid_0's multi_logloss: 1.48344\n",
      "[204]\tvalid_0's multi_logloss: 1.48323\n",
      "[205]\tvalid_0's multi_logloss: 1.48298\n",
      "[206]\tvalid_0's multi_logloss: 1.48275\n",
      "[207]\tvalid_0's multi_logloss: 1.48259\n",
      "[208]\tvalid_0's multi_logloss: 1.4824\n",
      "[209]\tvalid_0's multi_logloss: 1.48224\n",
      "[210]\tvalid_0's multi_logloss: 1.48198\n",
      "[211]\tvalid_0's multi_logloss: 1.48184\n",
      "[212]\tvalid_0's multi_logloss: 1.4816\n",
      "[213]\tvalid_0's multi_logloss: 1.48132\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[214]\tvalid_0's multi_logloss: 1.48106\n",
      "[215]\tvalid_0's multi_logloss: 1.48091\n",
      "[216]\tvalid_0's multi_logloss: 1.48077\n",
      "[217]\tvalid_0's multi_logloss: 1.48053\n",
      "[218]\tvalid_0's multi_logloss: 1.48034\n",
      "[219]\tvalid_0's multi_logloss: 1.4801\n",
      "[220]\tvalid_0's multi_logloss: 1.47984\n",
      "[221]\tvalid_0's multi_logloss: 1.47965\n",
      "[222]\tvalid_0's multi_logloss: 1.47941\n",
      "[223]\tvalid_0's multi_logloss: 1.47917\n",
      "[224]\tvalid_0's multi_logloss: 1.47896\n",
      "[225]\tvalid_0's multi_logloss: 1.47874\n",
      "[226]\tvalid_0's multi_logloss: 1.47856\n",
      "[227]\tvalid_0's multi_logloss: 1.47843\n",
      "[228]\tvalid_0's multi_logloss: 1.47817\n",
      "[229]\tvalid_0's multi_logloss: 1.47804\n",
      "[230]\tvalid_0's multi_logloss: 1.47791\n",
      "[231]\tvalid_0's multi_logloss: 1.47777\n",
      "[232]\tvalid_0's multi_logloss: 1.4776\n",
      "[233]\tvalid_0's multi_logloss: 1.47734\n",
      "[234]\tvalid_0's multi_logloss: 1.47713\n",
      "[235]\tvalid_0's multi_logloss: 1.47702\n",
      "[236]\tvalid_0's multi_logloss: 1.47685\n",
      "[237]\tvalid_0's multi_logloss: 1.4767\n",
      "[238]\tvalid_0's multi_logloss: 1.47657\n",
      "[239]\tvalid_0's multi_logloss: 1.47641\n",
      "[240]\tvalid_0's multi_logloss: 1.47621\n",
      "[241]\tvalid_0's multi_logloss: 1.47601\n",
      "[242]\tvalid_0's multi_logloss: 1.47581\n",
      "[243]\tvalid_0's multi_logloss: 1.47569\n",
      "[244]\tvalid_0's multi_logloss: 1.47546\n",
      "[245]\tvalid_0's multi_logloss: 1.47527\n",
      "[246]\tvalid_0's multi_logloss: 1.47507\n",
      "[247]\tvalid_0's multi_logloss: 1.47485\n",
      "[248]\tvalid_0's multi_logloss: 1.4747\n",
      "[249]\tvalid_0's multi_logloss: 1.47462\n",
      "[250]\tvalid_0's multi_logloss: 1.47443\n",
      "[251]\tvalid_0's multi_logloss: 1.47425\n",
      "[252]\tvalid_0's multi_logloss: 1.47407\n",
      "[253]\tvalid_0's multi_logloss: 1.47384\n",
      "[254]\tvalid_0's multi_logloss: 1.47356\n",
      "[255]\tvalid_0's multi_logloss: 1.47343\n",
      "[256]\tvalid_0's multi_logloss: 1.4733\n",
      "[257]\tvalid_0's multi_logloss: 1.47312\n",
      "[258]\tvalid_0's multi_logloss: 1.47287\n",
      "[259]\tvalid_0's multi_logloss: 1.4727\n",
      "[260]\tvalid_0's multi_logloss: 1.47263\n",
      "[261]\tvalid_0's multi_logloss: 1.47239\n",
      "[262]\tvalid_0's multi_logloss: 1.47217\n",
      "[263]\tvalid_0's multi_logloss: 1.47191\n",
      "[264]\tvalid_0's multi_logloss: 1.47175\n",
      "[265]\tvalid_0's multi_logloss: 1.47161\n",
      "[266]\tvalid_0's multi_logloss: 1.47149\n",
      "[267]\tvalid_0's multi_logloss: 1.47135\n",
      "[268]\tvalid_0's multi_logloss: 1.47117\n",
      "[269]\tvalid_0's multi_logloss: 1.47101\n",
      "[270]\tvalid_0's multi_logloss: 1.47084\n",
      "[271]\tvalid_0's multi_logloss: 1.47064\n",
      "[272]\tvalid_0's multi_logloss: 1.47057\n",
      "[273]\tvalid_0's multi_logloss: 1.47031\n",
      "[274]\tvalid_0's multi_logloss: 1.47022\n",
      "[275]\tvalid_0's multi_logloss: 1.47007\n",
      "[276]\tvalid_0's multi_logloss: 1.46993\n",
      "[277]\tvalid_0's multi_logloss: 1.4697\n",
      "[278]\tvalid_0's multi_logloss: 1.46949\n",
      "[279]\tvalid_0's multi_logloss: 1.4694\n",
      "[280]\tvalid_0's multi_logloss: 1.46928\n",
      "[281]\tvalid_0's multi_logloss: 1.4691\n",
      "[282]\tvalid_0's multi_logloss: 1.46895\n",
      "[283]\tvalid_0's multi_logloss: 1.46881\n",
      "[284]\tvalid_0's multi_logloss: 1.46869\n",
      "[285]\tvalid_0's multi_logloss: 1.46858\n",
      "[286]\tvalid_0's multi_logloss: 1.46843\n",
      "[287]\tvalid_0's multi_logloss: 1.46824\n",
      "[288]\tvalid_0's multi_logloss: 1.46801\n",
      "[289]\tvalid_0's multi_logloss: 1.46786\n",
      "[290]\tvalid_0's multi_logloss: 1.46772\n",
      "[291]\tvalid_0's multi_logloss: 1.46765\n",
      "[292]\tvalid_0's multi_logloss: 1.46747\n",
      "[293]\tvalid_0's multi_logloss: 1.46722\n",
      "[294]\tvalid_0's multi_logloss: 1.46693\n",
      "[295]\tvalid_0's multi_logloss: 1.46686\n",
      "[296]\tvalid_0's multi_logloss: 1.46676\n",
      "[297]\tvalid_0's multi_logloss: 1.46657\n",
      "[298]\tvalid_0's multi_logloss: 1.46651\n",
      "[299]\tvalid_0's multi_logloss: 1.46641\n",
      "[300]\tvalid_0's multi_logloss: 1.46628\n",
      "[301]\tvalid_0's multi_logloss: 1.46609\n",
      "[302]\tvalid_0's multi_logloss: 1.46587\n",
      "[303]\tvalid_0's multi_logloss: 1.46573\n",
      "[304]\tvalid_0's multi_logloss: 1.46556\n",
      "[305]\tvalid_0's multi_logloss: 1.46541\n",
      "[306]\tvalid_0's multi_logloss: 1.46533\n",
      "[307]\tvalid_0's multi_logloss: 1.46512\n",
      "[308]\tvalid_0's multi_logloss: 1.46489\n",
      "[309]\tvalid_0's multi_logloss: 1.46468\n",
      "[310]\tvalid_0's multi_logloss: 1.46455\n",
      "[311]\tvalid_0's multi_logloss: 1.46441\n",
      "[312]\tvalid_0's multi_logloss: 1.46426\n",
      "[313]\tvalid_0's multi_logloss: 1.46417\n",
      "[314]\tvalid_0's multi_logloss: 1.46405\n",
      "[315]\tvalid_0's multi_logloss: 1.46386\n",
      "[316]\tvalid_0's multi_logloss: 1.46378\n",
      "[317]\tvalid_0's multi_logloss: 1.46364\n",
      "[318]\tvalid_0's multi_logloss: 1.46356\n",
      "[319]\tvalid_0's multi_logloss: 1.46335\n",
      "[320]\tvalid_0's multi_logloss: 1.4632\n",
      "[321]\tvalid_0's multi_logloss: 1.46301\n",
      "[322]\tvalid_0's multi_logloss: 1.46285\n",
      "[323]\tvalid_0's multi_logloss: 1.46277\n",
      "[324]\tvalid_0's multi_logloss: 1.46271\n",
      "[325]\tvalid_0's multi_logloss: 1.46255\n",
      "[326]\tvalid_0's multi_logloss: 1.46239\n",
      "[327]\tvalid_0's multi_logloss: 1.46221\n",
      "[328]\tvalid_0's multi_logloss: 1.46205\n",
      "[329]\tvalid_0's multi_logloss: 1.4619\n",
      "[330]\tvalid_0's multi_logloss: 1.46178\n",
      "[331]\tvalid_0's multi_logloss: 1.46164\n",
      "[332]\tvalid_0's multi_logloss: 1.46154\n",
      "[333]\tvalid_0's multi_logloss: 1.46139\n",
      "[334]\tvalid_0's multi_logloss: 1.46121\n",
      "[335]\tvalid_0's multi_logloss: 1.46112\n",
      "[336]\tvalid_0's multi_logloss: 1.46096\n",
      "[337]\tvalid_0's multi_logloss: 1.46087\n",
      "[338]\tvalid_0's multi_logloss: 1.46066\n",
      "[339]\tvalid_0's multi_logloss: 1.46049\n",
      "[340]\tvalid_0's multi_logloss: 1.46035\n",
      "[341]\tvalid_0's multi_logloss: 1.4603\n",
      "[342]\tvalid_0's multi_logloss: 1.46019\n",
      "[343]\tvalid_0's multi_logloss: 1.46014\n",
      "[344]\tvalid_0's multi_logloss: 1.45991\n",
      "[345]\tvalid_0's multi_logloss: 1.45973\n",
      "[346]\tvalid_0's multi_logloss: 1.45962\n",
      "[347]\tvalid_0's multi_logloss: 1.45954\n",
      "[348]\tvalid_0's multi_logloss: 1.45938\n",
      "[349]\tvalid_0's multi_logloss: 1.45929\n",
      "[350]\tvalid_0's multi_logloss: 1.4592\n",
      "[351]\tvalid_0's multi_logloss: 1.45909\n",
      "[352]\tvalid_0's multi_logloss: 1.45889\n",
      "[353]\tvalid_0's multi_logloss: 1.45873\n",
      "[354]\tvalid_0's multi_logloss: 1.45862\n",
      "[355]\tvalid_0's multi_logloss: 1.45853\n",
      "[356]\tvalid_0's multi_logloss: 1.4584\n",
      "[357]\tvalid_0's multi_logloss: 1.45832\n",
      "[358]\tvalid_0's multi_logloss: 1.45815\n",
      "[359]\tvalid_0's multi_logloss: 1.45803\n",
      "[360]\tvalid_0's multi_logloss: 1.45785\n",
      "[361]\tvalid_0's multi_logloss: 1.45766\n",
      "[362]\tvalid_0's multi_logloss: 1.45753\n",
      "[363]\tvalid_0's multi_logloss: 1.45737\n",
      "[364]\tvalid_0's multi_logloss: 1.45727\n",
      "[365]\tvalid_0's multi_logloss: 1.45717\n",
      "[366]\tvalid_0's multi_logloss: 1.45705\n",
      "[367]\tvalid_0's multi_logloss: 1.45701\n",
      "[368]\tvalid_0's multi_logloss: 1.45684\n",
      "[369]\tvalid_0's multi_logloss: 1.45661\n",
      "[370]\tvalid_0's multi_logloss: 1.45638\n",
      "[371]\tvalid_0's multi_logloss: 1.45627\n",
      "[372]\tvalid_0's multi_logloss: 1.45616\n",
      "[373]\tvalid_0's multi_logloss: 1.45601\n",
      "[374]\tvalid_0's multi_logloss: 1.45591\n",
      "[375]\tvalid_0's multi_logloss: 1.45584\n",
      "[376]\tvalid_0's multi_logloss: 1.45571\n",
      "[377]\tvalid_0's multi_logloss: 1.45556\n",
      "[378]\tvalid_0's multi_logloss: 1.45548\n",
      "[379]\tvalid_0's multi_logloss: 1.45535\n",
      "[380]\tvalid_0's multi_logloss: 1.45523\n",
      "[381]\tvalid_0's multi_logloss: 1.45503\n",
      "[382]\tvalid_0's multi_logloss: 1.45491\n",
      "[383]\tvalid_0's multi_logloss: 1.45485\n",
      "[384]\tvalid_0's multi_logloss: 1.45476\n",
      "[385]\tvalid_0's multi_logloss: 1.45459\n",
      "[386]\tvalid_0's multi_logloss: 1.45441\n",
      "[387]\tvalid_0's multi_logloss: 1.45434\n",
      "[388]\tvalid_0's multi_logloss: 1.45422\n",
      "[389]\tvalid_0's multi_logloss: 1.45413\n",
      "[390]\tvalid_0's multi_logloss: 1.45405\n",
      "[391]\tvalid_0's multi_logloss: 1.4539\n",
      "[392]\tvalid_0's multi_logloss: 1.4538\n",
      "[393]\tvalid_0's multi_logloss: 1.45363\n",
      "[394]\tvalid_0's multi_logloss: 1.45353\n",
      "[395]\tvalid_0's multi_logloss: 1.45339\n",
      "[396]\tvalid_0's multi_logloss: 1.45322\n",
      "[397]\tvalid_0's multi_logloss: 1.45312\n",
      "[398]\tvalid_0's multi_logloss: 1.45296\n",
      "[399]\tvalid_0's multi_logloss: 1.45286\n",
      "[400]\tvalid_0's multi_logloss: 1.45274\n",
      "[401]\tvalid_0's multi_logloss: 1.45263\n",
      "[402]\tvalid_0's multi_logloss: 1.45252\n",
      "[403]\tvalid_0's multi_logloss: 1.45234\n",
      "[404]\tvalid_0's multi_logloss: 1.45222\n",
      "[405]\tvalid_0's multi_logloss: 1.45216\n",
      "[406]\tvalid_0's multi_logloss: 1.45206\n",
      "[407]\tvalid_0's multi_logloss: 1.45193\n",
      "[408]\tvalid_0's multi_logloss: 1.45183\n",
      "[409]\tvalid_0's multi_logloss: 1.45174\n",
      "[410]\tvalid_0's multi_logloss: 1.45163\n",
      "[411]\tvalid_0's multi_logloss: 1.45153\n",
      "[412]\tvalid_0's multi_logloss: 1.4514\n",
      "[413]\tvalid_0's multi_logloss: 1.45128\n",
      "[414]\tvalid_0's multi_logloss: 1.4511\n",
      "[415]\tvalid_0's multi_logloss: 1.451\n",
      "[416]\tvalid_0's multi_logloss: 1.45087\n",
      "[417]\tvalid_0's multi_logloss: 1.45074\n",
      "[418]\tvalid_0's multi_logloss: 1.45061\n",
      "[419]\tvalid_0's multi_logloss: 1.45055\n",
      "[420]\tvalid_0's multi_logloss: 1.4505\n",
      "[421]\tvalid_0's multi_logloss: 1.45037\n",
      "[422]\tvalid_0's multi_logloss: 1.45021\n",
      "[423]\tvalid_0's multi_logloss: 1.45016\n",
      "[424]\tvalid_0's multi_logloss: 1.4501\n",
      "[425]\tvalid_0's multi_logloss: 1.44995\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[426]\tvalid_0's multi_logloss: 1.44983\n",
      "[427]\tvalid_0's multi_logloss: 1.44967\n",
      "[428]\tvalid_0's multi_logloss: 1.44958\n",
      "[429]\tvalid_0's multi_logloss: 1.44944\n",
      "[430]\tvalid_0's multi_logloss: 1.44929\n",
      "[431]\tvalid_0's multi_logloss: 1.44915\n",
      "[432]\tvalid_0's multi_logloss: 1.44907\n",
      "[433]\tvalid_0's multi_logloss: 1.44894\n",
      "[434]\tvalid_0's multi_logloss: 1.44881\n",
      "[435]\tvalid_0's multi_logloss: 1.44867\n",
      "[436]\tvalid_0's multi_logloss: 1.4486\n",
      "[437]\tvalid_0's multi_logloss: 1.4485\n",
      "[438]\tvalid_0's multi_logloss: 1.4484\n",
      "[439]\tvalid_0's multi_logloss: 1.44827\n",
      "[440]\tvalid_0's multi_logloss: 1.4482\n",
      "[441]\tvalid_0's multi_logloss: 1.44806\n",
      "[442]\tvalid_0's multi_logloss: 1.44797\n",
      "[443]\tvalid_0's multi_logloss: 1.44783\n",
      "[444]\tvalid_0's multi_logloss: 1.4477\n",
      "[445]\tvalid_0's multi_logloss: 1.44757\n",
      "[446]\tvalid_0's multi_logloss: 1.44748\n",
      "[447]\tvalid_0's multi_logloss: 1.44728\n",
      "[448]\tvalid_0's multi_logloss: 1.44727\n",
      "[449]\tvalid_0's multi_logloss: 1.44718\n",
      "[450]\tvalid_0's multi_logloss: 1.44708\n",
      "[451]\tvalid_0's multi_logloss: 1.44698\n",
      "[452]\tvalid_0's multi_logloss: 1.44683\n",
      "[453]\tvalid_0's multi_logloss: 1.44669\n",
      "[454]\tvalid_0's multi_logloss: 1.44664\n",
      "[455]\tvalid_0's multi_logloss: 1.44661\n",
      "[456]\tvalid_0's multi_logloss: 1.4464\n",
      "[457]\tvalid_0's multi_logloss: 1.44635\n",
      "[458]\tvalid_0's multi_logloss: 1.44634\n",
      "[459]\tvalid_0's multi_logloss: 1.44624\n",
      "[460]\tvalid_0's multi_logloss: 1.44609\n",
      "[461]\tvalid_0's multi_logloss: 1.44604\n",
      "[462]\tvalid_0's multi_logloss: 1.44599\n",
      "[463]\tvalid_0's multi_logloss: 1.44589\n",
      "[464]\tvalid_0's multi_logloss: 1.44581\n",
      "[465]\tvalid_0's multi_logloss: 1.44578\n",
      "[466]\tvalid_0's multi_logloss: 1.44572\n",
      "[467]\tvalid_0's multi_logloss: 1.4456\n",
      "[468]\tvalid_0's multi_logloss: 1.44544\n",
      "[469]\tvalid_0's multi_logloss: 1.44546\n",
      "[470]\tvalid_0's multi_logloss: 1.44541\n",
      "[471]\tvalid_0's multi_logloss: 1.44525\n",
      "[472]\tvalid_0's multi_logloss: 1.44515\n",
      "[473]\tvalid_0's multi_logloss: 1.44506\n",
      "[474]\tvalid_0's multi_logloss: 1.44493\n",
      "[475]\tvalid_0's multi_logloss: 1.44488\n",
      "[476]\tvalid_0's multi_logloss: 1.44475\n",
      "[477]\tvalid_0's multi_logloss: 1.4446\n",
      "[478]\tvalid_0's multi_logloss: 1.44448\n",
      "[479]\tvalid_0's multi_logloss: 1.44443\n",
      "[480]\tvalid_0's multi_logloss: 1.44435\n",
      "[481]\tvalid_0's multi_logloss: 1.44429\n",
      "[482]\tvalid_0's multi_logloss: 1.44427\n",
      "[483]\tvalid_0's multi_logloss: 1.44421\n",
      "[484]\tvalid_0's multi_logloss: 1.44412\n",
      "[485]\tvalid_0's multi_logloss: 1.444\n",
      "[486]\tvalid_0's multi_logloss: 1.44391\n",
      "[487]\tvalid_0's multi_logloss: 1.44387\n",
      "[488]\tvalid_0's multi_logloss: 1.44378\n",
      "[489]\tvalid_0's multi_logloss: 1.44366\n",
      "[490]\tvalid_0's multi_logloss: 1.44354\n",
      "[491]\tvalid_0's multi_logloss: 1.44348\n",
      "[492]\tvalid_0's multi_logloss: 1.44339\n",
      "[493]\tvalid_0's multi_logloss: 1.44336\n",
      "[494]\tvalid_0's multi_logloss: 1.44332\n",
      "[495]\tvalid_0's multi_logloss: 1.44326\n",
      "[496]\tvalid_0's multi_logloss: 1.44325\n",
      "[497]\tvalid_0's multi_logloss: 1.44308\n",
      "[498]\tvalid_0's multi_logloss: 1.44306\n",
      "[499]\tvalid_0's multi_logloss: 1.44295\n",
      "[500]\tvalid_0's multi_logloss: 1.44288\n",
      "[501]\tvalid_0's multi_logloss: 1.44279\n",
      "[502]\tvalid_0's multi_logloss: 1.44273\n",
      "[503]\tvalid_0's multi_logloss: 1.4426\n",
      "[504]\tvalid_0's multi_logloss: 1.44255\n",
      "[505]\tvalid_0's multi_logloss: 1.44239\n",
      "[506]\tvalid_0's multi_logloss: 1.4423\n",
      "[507]\tvalid_0's multi_logloss: 1.44225\n",
      "[508]\tvalid_0's multi_logloss: 1.44218\n",
      "[509]\tvalid_0's multi_logloss: 1.44216\n",
      "[510]\tvalid_0's multi_logloss: 1.44212\n",
      "[511]\tvalid_0's multi_logloss: 1.44208\n",
      "[512]\tvalid_0's multi_logloss: 1.44203\n",
      "[513]\tvalid_0's multi_logloss: 1.44191\n",
      "[514]\tvalid_0's multi_logloss: 1.44178\n",
      "[515]\tvalid_0's multi_logloss: 1.4417\n",
      "[516]\tvalid_0's multi_logloss: 1.44158\n",
      "[517]\tvalid_0's multi_logloss: 1.44142\n",
      "[518]\tvalid_0's multi_logloss: 1.44139\n",
      "[519]\tvalid_0's multi_logloss: 1.44124\n",
      "[520]\tvalid_0's multi_logloss: 1.44122\n",
      "[521]\tvalid_0's multi_logloss: 1.44115\n",
      "[522]\tvalid_0's multi_logloss: 1.44108\n",
      "[523]\tvalid_0's multi_logloss: 1.44102\n",
      "[524]\tvalid_0's multi_logloss: 1.44096\n",
      "[525]\tvalid_0's multi_logloss: 1.44094\n",
      "[526]\tvalid_0's multi_logloss: 1.44091\n",
      "[527]\tvalid_0's multi_logloss: 1.44082\n",
      "[528]\tvalid_0's multi_logloss: 1.44064\n",
      "[529]\tvalid_0's multi_logloss: 1.44057\n",
      "[530]\tvalid_0's multi_logloss: 1.44046\n",
      "[531]\tvalid_0's multi_logloss: 1.44046\n",
      "[532]\tvalid_0's multi_logloss: 1.44036\n",
      "[533]\tvalid_0's multi_logloss: 1.44025\n",
      "[534]\tvalid_0's multi_logloss: 1.44018\n",
      "[535]\tvalid_0's multi_logloss: 1.44007\n",
      "[536]\tvalid_0's multi_logloss: 1.44\n",
      "[537]\tvalid_0's multi_logloss: 1.43994\n",
      "[538]\tvalid_0's multi_logloss: 1.43993\n",
      "[539]\tvalid_0's multi_logloss: 1.43989\n",
      "[540]\tvalid_0's multi_logloss: 1.43976\n",
      "[541]\tvalid_0's multi_logloss: 1.43968\n",
      "[542]\tvalid_0's multi_logloss: 1.4396\n",
      "[543]\tvalid_0's multi_logloss: 1.43952\n",
      "[544]\tvalid_0's multi_logloss: 1.43945\n",
      "[545]\tvalid_0's multi_logloss: 1.43938\n",
      "[546]\tvalid_0's multi_logloss: 1.43929\n",
      "[547]\tvalid_0's multi_logloss: 1.4393\n",
      "[548]\tvalid_0's multi_logloss: 1.4393\n",
      "[549]\tvalid_0's multi_logloss: 1.43919\n",
      "[550]\tvalid_0's multi_logloss: 1.43918\n",
      "[551]\tvalid_0's multi_logloss: 1.43918\n",
      "[552]\tvalid_0's multi_logloss: 1.43913\n",
      "[553]\tvalid_0's multi_logloss: 1.43898\n",
      "[554]\tvalid_0's multi_logloss: 1.43889\n",
      "[555]\tvalid_0's multi_logloss: 1.4388\n",
      "[556]\tvalid_0's multi_logloss: 1.43871\n",
      "[557]\tvalid_0's multi_logloss: 1.43862\n",
      "[558]\tvalid_0's multi_logloss: 1.43854\n",
      "[559]\tvalid_0's multi_logloss: 1.43851\n",
      "[560]\tvalid_0's multi_logloss: 1.43843\n",
      "[561]\tvalid_0's multi_logloss: 1.43836\n",
      "[562]\tvalid_0's multi_logloss: 1.4383\n",
      "[563]\tvalid_0's multi_logloss: 1.43817\n",
      "[564]\tvalid_0's multi_logloss: 1.43805\n",
      "[565]\tvalid_0's multi_logloss: 1.438\n",
      "[566]\tvalid_0's multi_logloss: 1.4379\n",
      "[567]\tvalid_0's multi_logloss: 1.43782\n",
      "[568]\tvalid_0's multi_logloss: 1.43781\n",
      "[569]\tvalid_0's multi_logloss: 1.43771\n",
      "[570]\tvalid_0's multi_logloss: 1.43764\n",
      "[571]\tvalid_0's multi_logloss: 1.43763\n",
      "[572]\tvalid_0's multi_logloss: 1.43754\n",
      "[573]\tvalid_0's multi_logloss: 1.43744\n",
      "[574]\tvalid_0's multi_logloss: 1.43741\n",
      "[575]\tvalid_0's multi_logloss: 1.43732\n",
      "[576]\tvalid_0's multi_logloss: 1.43717\n",
      "[577]\tvalid_0's multi_logloss: 1.4371\n",
      "[578]\tvalid_0's multi_logloss: 1.4371\n",
      "[579]\tvalid_0's multi_logloss: 1.43709\n",
      "[580]\tvalid_0's multi_logloss: 1.43708\n",
      "[581]\tvalid_0's multi_logloss: 1.43703\n",
      "[582]\tvalid_0's multi_logloss: 1.43699\n",
      "[583]\tvalid_0's multi_logloss: 1.43687\n",
      "[584]\tvalid_0's multi_logloss: 1.43684\n",
      "[585]\tvalid_0's multi_logloss: 1.43666\n",
      "[586]\tvalid_0's multi_logloss: 1.43665\n",
      "[587]\tvalid_0's multi_logloss: 1.43665\n",
      "[588]\tvalid_0's multi_logloss: 1.4366\n",
      "[589]\tvalid_0's multi_logloss: 1.43653\n",
      "[590]\tvalid_0's multi_logloss: 1.4364\n",
      "[591]\tvalid_0's multi_logloss: 1.43642\n",
      "[592]\tvalid_0's multi_logloss: 1.43643\n",
      "[593]\tvalid_0's multi_logloss: 1.43643\n",
      "[594]\tvalid_0's multi_logloss: 1.43634\n",
      "[595]\tvalid_0's multi_logloss: 1.43623\n",
      "[596]\tvalid_0's multi_logloss: 1.43617\n",
      "[597]\tvalid_0's multi_logloss: 1.43613\n",
      "[598]\tvalid_0's multi_logloss: 1.43604\n",
      "[599]\tvalid_0's multi_logloss: 1.43598\n",
      "[600]\tvalid_0's multi_logloss: 1.43585\n",
      "[601]\tvalid_0's multi_logloss: 1.43581\n",
      "[602]\tvalid_0's multi_logloss: 1.4357\n",
      "[603]\tvalid_0's multi_logloss: 1.43563\n",
      "[604]\tvalid_0's multi_logloss: 1.43554\n",
      "[605]\tvalid_0's multi_logloss: 1.43543\n",
      "[606]\tvalid_0's multi_logloss: 1.43536\n",
      "[607]\tvalid_0's multi_logloss: 1.43531\n",
      "[608]\tvalid_0's multi_logloss: 1.43528\n",
      "[609]\tvalid_0's multi_logloss: 1.43517\n",
      "[610]\tvalid_0's multi_logloss: 1.43515\n",
      "[611]\tvalid_0's multi_logloss: 1.43507\n",
      "[612]\tvalid_0's multi_logloss: 1.43494\n",
      "[613]\tvalid_0's multi_logloss: 1.43493\n",
      "[614]\tvalid_0's multi_logloss: 1.43485\n",
      "[615]\tvalid_0's multi_logloss: 1.4348\n",
      "[616]\tvalid_0's multi_logloss: 1.43476\n",
      "[617]\tvalid_0's multi_logloss: 1.43466\n",
      "[618]\tvalid_0's multi_logloss: 1.43466\n",
      "[619]\tvalid_0's multi_logloss: 1.43462\n",
      "[620]\tvalid_0's multi_logloss: 1.43449\n",
      "[621]\tvalid_0's multi_logloss: 1.43442\n",
      "[622]\tvalid_0's multi_logloss: 1.43435\n",
      "[623]\tvalid_0's multi_logloss: 1.43437\n",
      "[624]\tvalid_0's multi_logloss: 1.43428\n",
      "[625]\tvalid_0's multi_logloss: 1.43426\n",
      "[626]\tvalid_0's multi_logloss: 1.4342\n",
      "[627]\tvalid_0's multi_logloss: 1.43406\n",
      "[628]\tvalid_0's multi_logloss: 1.43398\n",
      "[629]\tvalid_0's multi_logloss: 1.43401\n",
      "[630]\tvalid_0's multi_logloss: 1.43387\n",
      "[631]\tvalid_0's multi_logloss: 1.43386\n",
      "[632]\tvalid_0's multi_logloss: 1.43382\n",
      "[633]\tvalid_0's multi_logloss: 1.43377\n",
      "[634]\tvalid_0's multi_logloss: 1.43373\n",
      "[635]\tvalid_0's multi_logloss: 1.43369\n",
      "[636]\tvalid_0's multi_logloss: 1.43366\n",
      "[637]\tvalid_0's multi_logloss: 1.43361\n",
      "[638]\tvalid_0's multi_logloss: 1.43355\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[639]\tvalid_0's multi_logloss: 1.43353\n",
      "[640]\tvalid_0's multi_logloss: 1.43342\n",
      "[641]\tvalid_0's multi_logloss: 1.43334\n",
      "[642]\tvalid_0's multi_logloss: 1.43327\n",
      "[643]\tvalid_0's multi_logloss: 1.43314\n",
      "[644]\tvalid_0's multi_logloss: 1.43315\n",
      "[645]\tvalid_0's multi_logloss: 1.43315\n",
      "[646]\tvalid_0's multi_logloss: 1.43308\n",
      "[647]\tvalid_0's multi_logloss: 1.43305\n",
      "[648]\tvalid_0's multi_logloss: 1.43297\n",
      "[649]\tvalid_0's multi_logloss: 1.43289\n",
      "[650]\tvalid_0's multi_logloss: 1.4328\n",
      "[651]\tvalid_0's multi_logloss: 1.4327\n",
      "[652]\tvalid_0's multi_logloss: 1.43259\n",
      "[653]\tvalid_0's multi_logloss: 1.4326\n",
      "[654]\tvalid_0's multi_logloss: 1.43257\n",
      "[655]\tvalid_0's multi_logloss: 1.4325\n",
      "[656]\tvalid_0's multi_logloss: 1.43243\n",
      "[657]\tvalid_0's multi_logloss: 1.43234\n",
      "[658]\tvalid_0's multi_logloss: 1.43229\n",
      "[659]\tvalid_0's multi_logloss: 1.43228\n",
      "[660]\tvalid_0's multi_logloss: 1.43224\n",
      "[661]\tvalid_0's multi_logloss: 1.43221\n",
      "[662]\tvalid_0's multi_logloss: 1.4321\n",
      "[663]\tvalid_0's multi_logloss: 1.43198\n",
      "[664]\tvalid_0's multi_logloss: 1.43201\n",
      "[665]\tvalid_0's multi_logloss: 1.43196\n",
      "[666]\tvalid_0's multi_logloss: 1.43194\n",
      "[667]\tvalid_0's multi_logloss: 1.43191\n",
      "[668]\tvalid_0's multi_logloss: 1.43185\n",
      "[669]\tvalid_0's multi_logloss: 1.43181\n",
      "[670]\tvalid_0's multi_logloss: 1.43172\n",
      "[671]\tvalid_0's multi_logloss: 1.43167\n",
      "[672]\tvalid_0's multi_logloss: 1.43161\n",
      "[673]\tvalid_0's multi_logloss: 1.43153\n",
      "[674]\tvalid_0's multi_logloss: 1.43145\n",
      "[675]\tvalid_0's multi_logloss: 1.43146\n",
      "[676]\tvalid_0's multi_logloss: 1.43145\n",
      "[677]\tvalid_0's multi_logloss: 1.43142\n",
      "[678]\tvalid_0's multi_logloss: 1.4314\n",
      "[679]\tvalid_0's multi_logloss: 1.43135\n",
      "[680]\tvalid_0's multi_logloss: 1.43134\n",
      "[681]\tvalid_0's multi_logloss: 1.43135\n",
      "[682]\tvalid_0's multi_logloss: 1.43125\n",
      "[683]\tvalid_0's multi_logloss: 1.43115\n",
      "[684]\tvalid_0's multi_logloss: 1.43112\n",
      "[685]\tvalid_0's multi_logloss: 1.43106\n",
      "[686]\tvalid_0's multi_logloss: 1.43102\n",
      "[687]\tvalid_0's multi_logloss: 1.43097\n",
      "[688]\tvalid_0's multi_logloss: 1.43097\n",
      "[689]\tvalid_0's multi_logloss: 1.431\n",
      "[690]\tvalid_0's multi_logloss: 1.43093\n",
      "[691]\tvalid_0's multi_logloss: 1.43089\n",
      "[692]\tvalid_0's multi_logloss: 1.43086\n",
      "[693]\tvalid_0's multi_logloss: 1.43085\n",
      "[694]\tvalid_0's multi_logloss: 1.43077\n",
      "[695]\tvalid_0's multi_logloss: 1.43069\n",
      "[696]\tvalid_0's multi_logloss: 1.43062\n",
      "[697]\tvalid_0's multi_logloss: 1.43056\n",
      "[698]\tvalid_0's multi_logloss: 1.43055\n",
      "[699]\tvalid_0's multi_logloss: 1.43055\n",
      "[700]\tvalid_0's multi_logloss: 1.43055\n",
      "[701]\tvalid_0's multi_logloss: 1.43049\n",
      "[702]\tvalid_0's multi_logloss: 1.43041\n",
      "[703]\tvalid_0's multi_logloss: 1.43038\n",
      "[704]\tvalid_0's multi_logloss: 1.43037\n",
      "[705]\tvalid_0's multi_logloss: 1.43034\n",
      "[706]\tvalid_0's multi_logloss: 1.43029\n",
      "[707]\tvalid_0's multi_logloss: 1.43019\n",
      "[708]\tvalid_0's multi_logloss: 1.43023\n",
      "[709]\tvalid_0's multi_logloss: 1.43013\n",
      "[710]\tvalid_0's multi_logloss: 1.43013\n",
      "[711]\tvalid_0's multi_logloss: 1.43013\n",
      "[712]\tvalid_0's multi_logloss: 1.43008\n",
      "[713]\tvalid_0's multi_logloss: 1.43007\n",
      "[714]\tvalid_0's multi_logloss: 1.43005\n",
      "[715]\tvalid_0's multi_logloss: 1.43\n",
      "[716]\tvalid_0's multi_logloss: 1.42993\n",
      "[717]\tvalid_0's multi_logloss: 1.42981\n",
      "[718]\tvalid_0's multi_logloss: 1.4298\n",
      "[719]\tvalid_0's multi_logloss: 1.42969\n",
      "[720]\tvalid_0's multi_logloss: 1.42966\n",
      "[721]\tvalid_0's multi_logloss: 1.42961\n",
      "[722]\tvalid_0's multi_logloss: 1.42955\n",
      "[723]\tvalid_0's multi_logloss: 1.42949\n",
      "[724]\tvalid_0's multi_logloss: 1.42947\n",
      "[725]\tvalid_0's multi_logloss: 1.42947\n",
      "[726]\tvalid_0's multi_logloss: 1.42946\n",
      "[727]\tvalid_0's multi_logloss: 1.42936\n",
      "[728]\tvalid_0's multi_logloss: 1.42931\n",
      "[729]\tvalid_0's multi_logloss: 1.4293\n",
      "[730]\tvalid_0's multi_logloss: 1.42923\n",
      "[731]\tvalid_0's multi_logloss: 1.4292\n",
      "[732]\tvalid_0's multi_logloss: 1.42913\n",
      "[733]\tvalid_0's multi_logloss: 1.42903\n",
      "[734]\tvalid_0's multi_logloss: 1.429\n",
      "[735]\tvalid_0's multi_logloss: 1.42899\n",
      "[736]\tvalid_0's multi_logloss: 1.42894\n",
      "[737]\tvalid_0's multi_logloss: 1.42887\n",
      "[738]\tvalid_0's multi_logloss: 1.4288\n",
      "[739]\tvalid_0's multi_logloss: 1.42878\n",
      "[740]\tvalid_0's multi_logloss: 1.4287\n",
      "[741]\tvalid_0's multi_logloss: 1.42866\n",
      "[742]\tvalid_0's multi_logloss: 1.4286\n",
      "[743]\tvalid_0's multi_logloss: 1.42853\n",
      "[744]\tvalid_0's multi_logloss: 1.42855\n",
      "[745]\tvalid_0's multi_logloss: 1.4285\n",
      "[746]\tvalid_0's multi_logloss: 1.42841\n",
      "[747]\tvalid_0's multi_logloss: 1.42837\n",
      "[748]\tvalid_0's multi_logloss: 1.42831\n",
      "[749]\tvalid_0's multi_logloss: 1.42833\n",
      "[750]\tvalid_0's multi_logloss: 1.42826\n",
      "[751]\tvalid_0's multi_logloss: 1.42828\n",
      "[752]\tvalid_0's multi_logloss: 1.42832\n",
      "[753]\tvalid_0's multi_logloss: 1.42824\n",
      "[754]\tvalid_0's multi_logloss: 1.42817\n",
      "[755]\tvalid_0's multi_logloss: 1.42807\n",
      "[756]\tvalid_0's multi_logloss: 1.42802\n",
      "[757]\tvalid_0's multi_logloss: 1.42802\n",
      "[758]\tvalid_0's multi_logloss: 1.42803\n",
      "[759]\tvalid_0's multi_logloss: 1.42798\n",
      "[760]\tvalid_0's multi_logloss: 1.42791\n",
      "[761]\tvalid_0's multi_logloss: 1.42782\n",
      "[762]\tvalid_0's multi_logloss: 1.42779\n",
      "[763]\tvalid_0's multi_logloss: 1.42775\n",
      "[764]\tvalid_0's multi_logloss: 1.42773\n",
      "[765]\tvalid_0's multi_logloss: 1.42765\n",
      "[766]\tvalid_0's multi_logloss: 1.42766\n",
      "[767]\tvalid_0's multi_logloss: 1.42757\n",
      "[768]\tvalid_0's multi_logloss: 1.42753\n",
      "[769]\tvalid_0's multi_logloss: 1.42746\n",
      "[770]\tvalid_0's multi_logloss: 1.42743\n",
      "[771]\tvalid_0's multi_logloss: 1.42742\n",
      "[772]\tvalid_0's multi_logloss: 1.42734\n",
      "[773]\tvalid_0's multi_logloss: 1.42733\n",
      "[774]\tvalid_0's multi_logloss: 1.42727\n",
      "[775]\tvalid_0's multi_logloss: 1.42723\n",
      "[776]\tvalid_0's multi_logloss: 1.42726\n",
      "[777]\tvalid_0's multi_logloss: 1.42723\n",
      "[778]\tvalid_0's multi_logloss: 1.42721\n",
      "[779]\tvalid_0's multi_logloss: 1.42718\n",
      "[780]\tvalid_0's multi_logloss: 1.4271\n",
      "[781]\tvalid_0's multi_logloss: 1.42711\n",
      "[782]\tvalid_0's multi_logloss: 1.42712\n",
      "[783]\tvalid_0's multi_logloss: 1.42706\n",
      "[784]\tvalid_0's multi_logloss: 1.42702\n",
      "[785]\tvalid_0's multi_logloss: 1.42699\n",
      "[786]\tvalid_0's multi_logloss: 1.42692\n",
      "[787]\tvalid_0's multi_logloss: 1.42683\n",
      "[788]\tvalid_0's multi_logloss: 1.42676\n",
      "[789]\tvalid_0's multi_logloss: 1.42672\n",
      "[790]\tvalid_0's multi_logloss: 1.42672\n",
      "[791]\tvalid_0's multi_logloss: 1.42669\n",
      "[792]\tvalid_0's multi_logloss: 1.42662\n",
      "[793]\tvalid_0's multi_logloss: 1.42656\n",
      "[794]\tvalid_0's multi_logloss: 1.42654\n",
      "[795]\tvalid_0's multi_logloss: 1.4265\n",
      "[796]\tvalid_0's multi_logloss: 1.42641\n",
      "[797]\tvalid_0's multi_logloss: 1.42647\n",
      "[798]\tvalid_0's multi_logloss: 1.42646\n",
      "[799]\tvalid_0's multi_logloss: 1.42639\n",
      "[800]\tvalid_0's multi_logloss: 1.42637\n",
      "[801]\tvalid_0's multi_logloss: 1.42636\n",
      "[802]\tvalid_0's multi_logloss: 1.4263\n",
      "[803]\tvalid_0's multi_logloss: 1.42631\n",
      "[804]\tvalid_0's multi_logloss: 1.42623\n",
      "[805]\tvalid_0's multi_logloss: 1.4262\n",
      "[806]\tvalid_0's multi_logloss: 1.42617\n",
      "[807]\tvalid_0's multi_logloss: 1.42607\n",
      "[808]\tvalid_0's multi_logloss: 1.42608\n",
      "[809]\tvalid_0's multi_logloss: 1.4261\n",
      "[810]\tvalid_0's multi_logloss: 1.42609\n",
      "[811]\tvalid_0's multi_logloss: 1.42603\n",
      "[812]\tvalid_0's multi_logloss: 1.42599\n",
      "[813]\tvalid_0's multi_logloss: 1.42592\n",
      "[814]\tvalid_0's multi_logloss: 1.42588\n",
      "[815]\tvalid_0's multi_logloss: 1.42581\n",
      "[816]\tvalid_0's multi_logloss: 1.4257\n",
      "[817]\tvalid_0's multi_logloss: 1.42563\n",
      "[818]\tvalid_0's multi_logloss: 1.42563\n",
      "[819]\tvalid_0's multi_logloss: 1.42558\n",
      "[820]\tvalid_0's multi_logloss: 1.42554\n",
      "[821]\tvalid_0's multi_logloss: 1.42554\n",
      "[822]\tvalid_0's multi_logloss: 1.42547\n",
      "[823]\tvalid_0's multi_logloss: 1.42544\n",
      "[824]\tvalid_0's multi_logloss: 1.42535\n",
      "[825]\tvalid_0's multi_logloss: 1.42532\n",
      "[826]\tvalid_0's multi_logloss: 1.42531\n",
      "[827]\tvalid_0's multi_logloss: 1.42528\n",
      "[828]\tvalid_0's multi_logloss: 1.42524\n",
      "[829]\tvalid_0's multi_logloss: 1.42518\n",
      "[830]\tvalid_0's multi_logloss: 1.4252\n",
      "[831]\tvalid_0's multi_logloss: 1.42515\n",
      "[832]\tvalid_0's multi_logloss: 1.42514\n",
      "[833]\tvalid_0's multi_logloss: 1.42513\n",
      "[834]\tvalid_0's multi_logloss: 1.42508\n",
      "[835]\tvalid_0's multi_logloss: 1.42509\n",
      "[836]\tvalid_0's multi_logloss: 1.4251\n",
      "[837]\tvalid_0's multi_logloss: 1.4251\n",
      "[838]\tvalid_0's multi_logloss: 1.42503\n",
      "[839]\tvalid_0's multi_logloss: 1.42499\n",
      "[840]\tvalid_0's multi_logloss: 1.42494\n",
      "[841]\tvalid_0's multi_logloss: 1.42492\n",
      "[842]\tvalid_0's multi_logloss: 1.42491\n",
      "[843]\tvalid_0's multi_logloss: 1.42493\n",
      "[844]\tvalid_0's multi_logloss: 1.42491\n",
      "[845]\tvalid_0's multi_logloss: 1.42489\n",
      "[846]\tvalid_0's multi_logloss: 1.42481\n",
      "[847]\tvalid_0's multi_logloss: 1.42476\n",
      "[848]\tvalid_0's multi_logloss: 1.42472\n",
      "[849]\tvalid_0's multi_logloss: 1.42469\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[850]\tvalid_0's multi_logloss: 1.42459\n",
      "[851]\tvalid_0's multi_logloss: 1.42457\n",
      "[852]\tvalid_0's multi_logloss: 1.42458\n",
      "[853]\tvalid_0's multi_logloss: 1.42456\n",
      "[854]\tvalid_0's multi_logloss: 1.42451\n",
      "[855]\tvalid_0's multi_logloss: 1.42447\n",
      "[856]\tvalid_0's multi_logloss: 1.42444\n",
      "[857]\tvalid_0's multi_logloss: 1.42437\n",
      "[858]\tvalid_0's multi_logloss: 1.4243\n",
      "[859]\tvalid_0's multi_logloss: 1.42431\n",
      "[860]\tvalid_0's multi_logloss: 1.42429\n",
      "[861]\tvalid_0's multi_logloss: 1.42426\n",
      "[862]\tvalid_0's multi_logloss: 1.42417\n",
      "[863]\tvalid_0's multi_logloss: 1.42416\n",
      "[864]\tvalid_0's multi_logloss: 1.42415\n",
      "[865]\tvalid_0's multi_logloss: 1.42409\n",
      "[866]\tvalid_0's multi_logloss: 1.42412\n",
      "[867]\tvalid_0's multi_logloss: 1.42405\n",
      "[868]\tvalid_0's multi_logloss: 1.42401\n",
      "[869]\tvalid_0's multi_logloss: 1.42405\n",
      "[870]\tvalid_0's multi_logloss: 1.42403\n",
      "[871]\tvalid_0's multi_logloss: 1.424\n",
      "[872]\tvalid_0's multi_logloss: 1.42397\n",
      "[873]\tvalid_0's multi_logloss: 1.42391\n",
      "[874]\tvalid_0's multi_logloss: 1.42391\n",
      "[875]\tvalid_0's multi_logloss: 1.42392\n",
      "[876]\tvalid_0's multi_logloss: 1.42384\n",
      "[877]\tvalid_0's multi_logloss: 1.42381\n",
      "[878]\tvalid_0's multi_logloss: 1.42382\n",
      "[879]\tvalid_0's multi_logloss: 1.42379\n",
      "[880]\tvalid_0's multi_logloss: 1.42382\n",
      "[881]\tvalid_0's multi_logloss: 1.42375\n",
      "[882]\tvalid_0's multi_logloss: 1.42372\n",
      "[883]\tvalid_0's multi_logloss: 1.42369\n",
      "[884]\tvalid_0's multi_logloss: 1.42363\n",
      "[885]\tvalid_0's multi_logloss: 1.4236\n",
      "[886]\tvalid_0's multi_logloss: 1.42359\n",
      "[887]\tvalid_0's multi_logloss: 1.42354\n",
      "[888]\tvalid_0's multi_logloss: 1.4235\n",
      "[889]\tvalid_0's multi_logloss: 1.42351\n",
      "[890]\tvalid_0's multi_logloss: 1.42351\n",
      "[891]\tvalid_0's multi_logloss: 1.42346\n",
      "[892]\tvalid_0's multi_logloss: 1.42343\n",
      "[893]\tvalid_0's multi_logloss: 1.42337\n",
      "[894]\tvalid_0's multi_logloss: 1.4234\n",
      "[895]\tvalid_0's multi_logloss: 1.42332\n",
      "[896]\tvalid_0's multi_logloss: 1.42327\n",
      "[897]\tvalid_0's multi_logloss: 1.42321\n",
      "[898]\tvalid_0's multi_logloss: 1.42316\n",
      "[899]\tvalid_0's multi_logloss: 1.42313\n",
      "[900]\tvalid_0's multi_logloss: 1.42312\n",
      "[901]\tvalid_0's multi_logloss: 1.42308\n",
      "[902]\tvalid_0's multi_logloss: 1.42306\n",
      "[903]\tvalid_0's multi_logloss: 1.42307\n",
      "[904]\tvalid_0's multi_logloss: 1.42305\n",
      "[905]\tvalid_0's multi_logloss: 1.42304\n",
      "[906]\tvalid_0's multi_logloss: 1.42301\n",
      "[907]\tvalid_0's multi_logloss: 1.42299\n",
      "[908]\tvalid_0's multi_logloss: 1.42288\n",
      "[909]\tvalid_0's multi_logloss: 1.42286\n",
      "[910]\tvalid_0's multi_logloss: 1.4228\n",
      "[911]\tvalid_0's multi_logloss: 1.42282\n",
      "[912]\tvalid_0's multi_logloss: 1.42274\n",
      "[913]\tvalid_0's multi_logloss: 1.42269\n",
      "[914]\tvalid_0's multi_logloss: 1.42262\n",
      "[915]\tvalid_0's multi_logloss: 1.42259\n",
      "[916]\tvalid_0's multi_logloss: 1.42255\n",
      "[917]\tvalid_0's multi_logloss: 1.42253\n",
      "[918]\tvalid_0's multi_logloss: 1.42252\n",
      "[919]\tvalid_0's multi_logloss: 1.42252\n",
      "[920]\tvalid_0's multi_logloss: 1.42247\n",
      "[921]\tvalid_0's multi_logloss: 1.42253\n",
      "[922]\tvalid_0's multi_logloss: 1.42247\n",
      "[923]\tvalid_0's multi_logloss: 1.42243\n",
      "[924]\tvalid_0's multi_logloss: 1.42233\n",
      "[925]\tvalid_0's multi_logloss: 1.42231\n",
      "[926]\tvalid_0's multi_logloss: 1.42224\n",
      "[927]\tvalid_0's multi_logloss: 1.42221\n",
      "[928]\tvalid_0's multi_logloss: 1.42215\n",
      "[929]\tvalid_0's multi_logloss: 1.42214\n",
      "[930]\tvalid_0's multi_logloss: 1.42208\n",
      "[931]\tvalid_0's multi_logloss: 1.42203\n",
      "[932]\tvalid_0's multi_logloss: 1.422\n",
      "[933]\tvalid_0's multi_logloss: 1.42196\n",
      "[934]\tvalid_0's multi_logloss: 1.42196\n",
      "[935]\tvalid_0's multi_logloss: 1.42193\n",
      "[936]\tvalid_0's multi_logloss: 1.42189\n",
      "[937]\tvalid_0's multi_logloss: 1.42197\n",
      "[938]\tvalid_0's multi_logloss: 1.4219\n",
      "[939]\tvalid_0's multi_logloss: 1.42181\n",
      "[940]\tvalid_0's multi_logloss: 1.42179\n",
      "[941]\tvalid_0's multi_logloss: 1.42174\n",
      "[942]\tvalid_0's multi_logloss: 1.42172\n",
      "[943]\tvalid_0's multi_logloss: 1.42175\n",
      "[944]\tvalid_0's multi_logloss: 1.4217\n",
      "[945]\tvalid_0's multi_logloss: 1.42172\n",
      "[946]\tvalid_0's multi_logloss: 1.42176\n",
      "[947]\tvalid_0's multi_logloss: 1.42169\n",
      "[948]\tvalid_0's multi_logloss: 1.42162\n",
      "[949]\tvalid_0's multi_logloss: 1.42157\n",
      "[950]\tvalid_0's multi_logloss: 1.42151\n",
      "[951]\tvalid_0's multi_logloss: 1.42153\n",
      "[952]\tvalid_0's multi_logloss: 1.42149\n",
      "[953]\tvalid_0's multi_logloss: 1.42147\n",
      "[954]\tvalid_0's multi_logloss: 1.42145\n",
      "[955]\tvalid_0's multi_logloss: 1.42144\n",
      "[956]\tvalid_0's multi_logloss: 1.4214\n",
      "[957]\tvalid_0's multi_logloss: 1.42136\n",
      "[958]\tvalid_0's multi_logloss: 1.42131\n",
      "[959]\tvalid_0's multi_logloss: 1.42131\n",
      "[960]\tvalid_0's multi_logloss: 1.42128\n",
      "[961]\tvalid_0's multi_logloss: 1.42123\n",
      "[962]\tvalid_0's multi_logloss: 1.42123\n",
      "[963]\tvalid_0's multi_logloss: 1.42121\n",
      "[964]\tvalid_0's multi_logloss: 1.42123\n",
      "[965]\tvalid_0's multi_logloss: 1.42118\n",
      "[966]\tvalid_0's multi_logloss: 1.42115\n",
      "[967]\tvalid_0's multi_logloss: 1.42114\n",
      "[968]\tvalid_0's multi_logloss: 1.42108\n",
      "[969]\tvalid_0's multi_logloss: 1.42104\n",
      "[970]\tvalid_0's multi_logloss: 1.42106\n",
      "[971]\tvalid_0's multi_logloss: 1.42102\n",
      "[972]\tvalid_0's multi_logloss: 1.42107\n",
      "[973]\tvalid_0's multi_logloss: 1.42099\n",
      "[974]\tvalid_0's multi_logloss: 1.42097\n",
      "[975]\tvalid_0's multi_logloss: 1.42098\n",
      "[976]\tvalid_0's multi_logloss: 1.42093\n",
      "[977]\tvalid_0's multi_logloss: 1.42087\n",
      "[978]\tvalid_0's multi_logloss: 1.42089\n",
      "[979]\tvalid_0's multi_logloss: 1.4209\n",
      "[980]\tvalid_0's multi_logloss: 1.42085\n",
      "[981]\tvalid_0's multi_logloss: 1.42085\n",
      "[982]\tvalid_0's multi_logloss: 1.42083\n",
      "[983]\tvalid_0's multi_logloss: 1.42077\n",
      "[984]\tvalid_0's multi_logloss: 1.42075\n",
      "[985]\tvalid_0's multi_logloss: 1.42074\n",
      "[986]\tvalid_0's multi_logloss: 1.42066\n",
      "[987]\tvalid_0's multi_logloss: 1.42059\n",
      "[988]\tvalid_0's multi_logloss: 1.42061\n",
      "[989]\tvalid_0's multi_logloss: 1.42061\n",
      "[990]\tvalid_0's multi_logloss: 1.42056\n",
      "[991]\tvalid_0's multi_logloss: 1.42055\n",
      "[992]\tvalid_0's multi_logloss: 1.42045\n",
      "[993]\tvalid_0's multi_logloss: 1.42046\n",
      "[994]\tvalid_0's multi_logloss: 1.42042\n",
      "[995]\tvalid_0's multi_logloss: 1.42043\n",
      "[996]\tvalid_0's multi_logloss: 1.4204\n",
      "[997]\tvalid_0's multi_logloss: 1.42035\n",
      "[998]\tvalid_0's multi_logloss: 1.42036\n",
      "[999]\tvalid_0's multi_logloss: 1.42035\n",
      "[1000]\tvalid_0's multi_logloss: 1.42034\n",
      "[1001]\tvalid_0's multi_logloss: 1.42036\n",
      "[1002]\tvalid_0's multi_logloss: 1.42031\n",
      "[1003]\tvalid_0's multi_logloss: 1.42032\n",
      "[1004]\tvalid_0's multi_logloss: 1.42027\n",
      "[1005]\tvalid_0's multi_logloss: 1.42028\n",
      "[1006]\tvalid_0's multi_logloss: 1.4202\n",
      "[1007]\tvalid_0's multi_logloss: 1.42019\n",
      "[1008]\tvalid_0's multi_logloss: 1.42018\n",
      "[1009]\tvalid_0's multi_logloss: 1.42018\n",
      "[1010]\tvalid_0's multi_logloss: 1.42015\n",
      "[1011]\tvalid_0's multi_logloss: 1.42011\n",
      "[1012]\tvalid_0's multi_logloss: 1.42004\n",
      "[1013]\tvalid_0's multi_logloss: 1.42007\n",
      "[1014]\tvalid_0's multi_logloss: 1.42005\n",
      "[1015]\tvalid_0's multi_logloss: 1.42009\n",
      "[1016]\tvalid_0's multi_logloss: 1.42008\n",
      "[1017]\tvalid_0's multi_logloss: 1.42003\n",
      "[1018]\tvalid_0's multi_logloss: 1.41998\n",
      "[1019]\tvalid_0's multi_logloss: 1.41998\n",
      "[1020]\tvalid_0's multi_logloss: 1.42\n",
      "[1021]\tvalid_0's multi_logloss: 1.41997\n",
      "[1022]\tvalid_0's multi_logloss: 1.41992\n",
      "[1023]\tvalid_0's multi_logloss: 1.41992\n",
      "[1024]\tvalid_0's multi_logloss: 1.4199\n",
      "[1025]\tvalid_0's multi_logloss: 1.41993\n",
      "[1026]\tvalid_0's multi_logloss: 1.41989\n",
      "[1027]\tvalid_0's multi_logloss: 1.4199\n",
      "[1028]\tvalid_0's multi_logloss: 1.41983\n",
      "[1029]\tvalid_0's multi_logloss: 1.41979\n",
      "[1030]\tvalid_0's multi_logloss: 1.4198\n",
      "[1031]\tvalid_0's multi_logloss: 1.41978\n",
      "[1032]\tvalid_0's multi_logloss: 1.41975\n",
      "[1033]\tvalid_0's multi_logloss: 1.41973\n",
      "[1034]\tvalid_0's multi_logloss: 1.41967\n",
      "[1035]\tvalid_0's multi_logloss: 1.41967\n",
      "[1036]\tvalid_0's multi_logloss: 1.41964\n",
      "[1037]\tvalid_0's multi_logloss: 1.41965\n",
      "[1038]\tvalid_0's multi_logloss: 1.4196\n",
      "[1039]\tvalid_0's multi_logloss: 1.41961\n",
      "[1040]\tvalid_0's multi_logloss: 1.41962\n",
      "[1041]\tvalid_0's multi_logloss: 1.4196\n",
      "[1042]\tvalid_0's multi_logloss: 1.41958\n",
      "[1043]\tvalid_0's multi_logloss: 1.41953\n",
      "[1044]\tvalid_0's multi_logloss: 1.41947\n",
      "[1045]\tvalid_0's multi_logloss: 1.41942\n",
      "[1046]\tvalid_0's multi_logloss: 1.41936\n",
      "[1047]\tvalid_0's multi_logloss: 1.41938\n",
      "[1048]\tvalid_0's multi_logloss: 1.41932\n",
      "[1049]\tvalid_0's multi_logloss: 1.41928\n",
      "[1050]\tvalid_0's multi_logloss: 1.41925\n",
      "[1051]\tvalid_0's multi_logloss: 1.41912\n",
      "[1052]\tvalid_0's multi_logloss: 1.41908\n",
      "[1053]\tvalid_0's multi_logloss: 1.41908\n",
      "[1054]\tvalid_0's multi_logloss: 1.41901\n",
      "[1055]\tvalid_0's multi_logloss: 1.41903\n",
      "[1056]\tvalid_0's multi_logloss: 1.41896\n",
      "[1057]\tvalid_0's multi_logloss: 1.4189\n",
      "[1058]\tvalid_0's multi_logloss: 1.41889\n",
      "[1059]\tvalid_0's multi_logloss: 1.41891\n",
      "[1060]\tvalid_0's multi_logloss: 1.41895\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1061]\tvalid_0's multi_logloss: 1.41892\n",
      "[1062]\tvalid_0's multi_logloss: 1.41892\n",
      "[1063]\tvalid_0's multi_logloss: 1.41888\n",
      "[1064]\tvalid_0's multi_logloss: 1.41886\n",
      "[1065]\tvalid_0's multi_logloss: 1.41887\n",
      "[1066]\tvalid_0's multi_logloss: 1.41888\n",
      "[1067]\tvalid_0's multi_logloss: 1.41877\n",
      "[1068]\tvalid_0's multi_logloss: 1.41873\n",
      "[1069]\tvalid_0's multi_logloss: 1.41871\n",
      "[1070]\tvalid_0's multi_logloss: 1.41868\n",
      "[1071]\tvalid_0's multi_logloss: 1.41868\n",
      "[1072]\tvalid_0's multi_logloss: 1.41868\n",
      "[1073]\tvalid_0's multi_logloss: 1.41864\n",
      "[1074]\tvalid_0's multi_logloss: 1.41862\n",
      "[1075]\tvalid_0's multi_logloss: 1.41857\n",
      "[1076]\tvalid_0's multi_logloss: 1.41848\n",
      "[1077]\tvalid_0's multi_logloss: 1.41841\n",
      "[1078]\tvalid_0's multi_logloss: 1.41846\n",
      "[1079]\tvalid_0's multi_logloss: 1.41844\n",
      "[1080]\tvalid_0's multi_logloss: 1.41842\n",
      "[1081]\tvalid_0's multi_logloss: 1.41838\n",
      "[1082]\tvalid_0's multi_logloss: 1.41834\n",
      "[1083]\tvalid_0's multi_logloss: 1.41831\n",
      "[1084]\tvalid_0's multi_logloss: 1.41822\n",
      "[1085]\tvalid_0's multi_logloss: 1.41818\n",
      "[1086]\tvalid_0's multi_logloss: 1.41822\n",
      "[1087]\tvalid_0's multi_logloss: 1.41819\n",
      "[1088]\tvalid_0's multi_logloss: 1.41814\n",
      "[1089]\tvalid_0's multi_logloss: 1.41809\n",
      "[1090]\tvalid_0's multi_logloss: 1.41808\n",
      "[1091]\tvalid_0's multi_logloss: 1.41805\n",
      "[1092]\tvalid_0's multi_logloss: 1.41807\n",
      "[1093]\tvalid_0's multi_logloss: 1.41811\n",
      "[1094]\tvalid_0's multi_logloss: 1.41813\n",
      "[1095]\tvalid_0's multi_logloss: 1.41809\n",
      "[1096]\tvalid_0's multi_logloss: 1.4181\n",
      "[1097]\tvalid_0's multi_logloss: 1.41805\n",
      "[1098]\tvalid_0's multi_logloss: 1.41803\n",
      "[1099]\tvalid_0's multi_logloss: 1.41802\n",
      "[1100]\tvalid_0's multi_logloss: 1.41804\n",
      "[1101]\tvalid_0's multi_logloss: 1.41797\n",
      "[1102]\tvalid_0's multi_logloss: 1.41792\n",
      "[1103]\tvalid_0's multi_logloss: 1.41789\n",
      "[1104]\tvalid_0's multi_logloss: 1.41784\n",
      "[1105]\tvalid_0's multi_logloss: 1.41781\n",
      "[1106]\tvalid_0's multi_logloss: 1.41781\n",
      "[1107]\tvalid_0's multi_logloss: 1.41776\n",
      "[1108]\tvalid_0's multi_logloss: 1.41771\n",
      "[1109]\tvalid_0's multi_logloss: 1.41771\n",
      "[1110]\tvalid_0's multi_logloss: 1.41767\n",
      "[1111]\tvalid_0's multi_logloss: 1.41759\n",
      "[1112]\tvalid_0's multi_logloss: 1.41753\n",
      "[1113]\tvalid_0's multi_logloss: 1.4175\n",
      "[1114]\tvalid_0's multi_logloss: 1.41748\n",
      "[1115]\tvalid_0's multi_logloss: 1.41749\n",
      "[1116]\tvalid_0's multi_logloss: 1.41752\n",
      "[1117]\tvalid_0's multi_logloss: 1.41752\n",
      "[1118]\tvalid_0's multi_logloss: 1.41753\n",
      "[1119]\tvalid_0's multi_logloss: 1.4175\n",
      "[1120]\tvalid_0's multi_logloss: 1.41749\n",
      "[1121]\tvalid_0's multi_logloss: 1.41752\n",
      "[1122]\tvalid_0's multi_logloss: 1.41752\n",
      "[1123]\tvalid_0's multi_logloss: 1.41748\n",
      "[1124]\tvalid_0's multi_logloss: 1.41748\n",
      "[1125]\tvalid_0's multi_logloss: 1.41744\n",
      "[1126]\tvalid_0's multi_logloss: 1.41737\n",
      "[1127]\tvalid_0's multi_logloss: 1.41733\n",
      "[1128]\tvalid_0's multi_logloss: 1.41726\n",
      "[1129]\tvalid_0's multi_logloss: 1.41726\n",
      "[1130]\tvalid_0's multi_logloss: 1.41723\n",
      "[1131]\tvalid_0's multi_logloss: 1.4172\n",
      "[1132]\tvalid_0's multi_logloss: 1.41717\n",
      "[1133]\tvalid_0's multi_logloss: 1.41711\n",
      "[1134]\tvalid_0's multi_logloss: 1.41712\n",
      "[1135]\tvalid_0's multi_logloss: 1.41712\n",
      "[1136]\tvalid_0's multi_logloss: 1.4171\n",
      "[1137]\tvalid_0's multi_logloss: 1.41712\n",
      "[1138]\tvalid_0's multi_logloss: 1.41705\n",
      "[1139]\tvalid_0's multi_logloss: 1.41702\n",
      "[1140]\tvalid_0's multi_logloss: 1.41706\n",
      "[1141]\tvalid_0's multi_logloss: 1.41703\n",
      "[1142]\tvalid_0's multi_logloss: 1.41699\n",
      "[1143]\tvalid_0's multi_logloss: 1.41698\n",
      "[1144]\tvalid_0's multi_logloss: 1.41693\n",
      "[1145]\tvalid_0's multi_logloss: 1.41689\n",
      "[1146]\tvalid_0's multi_logloss: 1.41687\n",
      "[1147]\tvalid_0's multi_logloss: 1.41686\n",
      "[1148]\tvalid_0's multi_logloss: 1.41688\n",
      "[1149]\tvalid_0's multi_logloss: 1.41684\n",
      "[1150]\tvalid_0's multi_logloss: 1.41678\n",
      "[1151]\tvalid_0's multi_logloss: 1.41678\n",
      "[1152]\tvalid_0's multi_logloss: 1.41682\n",
      "[1153]\tvalid_0's multi_logloss: 1.41675\n",
      "[1154]\tvalid_0's multi_logloss: 1.41673\n",
      "[1155]\tvalid_0's multi_logloss: 1.4167\n",
      "[1156]\tvalid_0's multi_logloss: 1.41668\n",
      "[1157]\tvalid_0's multi_logloss: 1.4167\n",
      "[1158]\tvalid_0's multi_logloss: 1.41673\n",
      "[1159]\tvalid_0's multi_logloss: 1.41672\n",
      "[1160]\tvalid_0's multi_logloss: 1.4167\n",
      "[1161]\tvalid_0's multi_logloss: 1.41662\n",
      "[1162]\tvalid_0's multi_logloss: 1.41653\n",
      "[1163]\tvalid_0's multi_logloss: 1.41652\n",
      "[1164]\tvalid_0's multi_logloss: 1.41659\n",
      "[1165]\tvalid_0's multi_logloss: 1.41662\n",
      "[1166]\tvalid_0's multi_logloss: 1.41654\n",
      "[1167]\tvalid_0's multi_logloss: 1.41653\n",
      "[1168]\tvalid_0's multi_logloss: 1.41649\n",
      "[1169]\tvalid_0's multi_logloss: 1.41649\n",
      "[1170]\tvalid_0's multi_logloss: 1.41644\n",
      "[1171]\tvalid_0's multi_logloss: 1.41644\n",
      "[1172]\tvalid_0's multi_logloss: 1.41637\n",
      "[1173]\tvalid_0's multi_logloss: 1.41638\n",
      "[1174]\tvalid_0's multi_logloss: 1.41636\n",
      "[1175]\tvalid_0's multi_logloss: 1.41638\n",
      "[1176]\tvalid_0's multi_logloss: 1.41636\n",
      "[1177]\tvalid_0's multi_logloss: 1.41629\n",
      "[1178]\tvalid_0's multi_logloss: 1.41636\n",
      "[1179]\tvalid_0's multi_logloss: 1.41633\n",
      "[1180]\tvalid_0's multi_logloss: 1.41633\n",
      "[1181]\tvalid_0's multi_logloss: 1.41628\n",
      "[1182]\tvalid_0's multi_logloss: 1.41634\n",
      "[1183]\tvalid_0's multi_logloss: 1.41635\n",
      "[1184]\tvalid_0's multi_logloss: 1.41635\n",
      "[1185]\tvalid_0's multi_logloss: 1.41632\n",
      "[1186]\tvalid_0's multi_logloss: 1.41628\n",
      "[1187]\tvalid_0's multi_logloss: 1.41627\n",
      "[1188]\tvalid_0's multi_logloss: 1.41622\n",
      "[1189]\tvalid_0's multi_logloss: 1.4162\n",
      "[1190]\tvalid_0's multi_logloss: 1.41614\n",
      "[1191]\tvalid_0's multi_logloss: 1.41614\n",
      "[1192]\tvalid_0's multi_logloss: 1.41612\n",
      "[1193]\tvalid_0's multi_logloss: 1.41615\n",
      "[1194]\tvalid_0's multi_logloss: 1.41612\n",
      "[1195]\tvalid_0's multi_logloss: 1.41609\n",
      "[1196]\tvalid_0's multi_logloss: 1.41609\n",
      "[1197]\tvalid_0's multi_logloss: 1.41604\n",
      "[1198]\tvalid_0's multi_logloss: 1.416\n",
      "[1199]\tvalid_0's multi_logloss: 1.41595\n",
      "[1200]\tvalid_0's multi_logloss: 1.41585\n",
      "[1201]\tvalid_0's multi_logloss: 1.41588\n",
      "[1202]\tvalid_0's multi_logloss: 1.41588\n",
      "[1203]\tvalid_0's multi_logloss: 1.41586\n",
      "[1204]\tvalid_0's multi_logloss: 1.41582\n",
      "[1205]\tvalid_0's multi_logloss: 1.41585\n",
      "[1206]\tvalid_0's multi_logloss: 1.41582\n",
      "[1207]\tvalid_0's multi_logloss: 1.41584\n",
      "[1208]\tvalid_0's multi_logloss: 1.41586\n",
      "[1209]\tvalid_0's multi_logloss: 1.41581\n",
      "[1210]\tvalid_0's multi_logloss: 1.41578\n",
      "[1211]\tvalid_0's multi_logloss: 1.41583\n",
      "[1212]\tvalid_0's multi_logloss: 1.41581\n",
      "[1213]\tvalid_0's multi_logloss: 1.41583\n",
      "[1214]\tvalid_0's multi_logloss: 1.41582\n",
      "[1215]\tvalid_0's multi_logloss: 1.41579\n",
      "[1216]\tvalid_0's multi_logloss: 1.41583\n",
      "[1217]\tvalid_0's multi_logloss: 1.41579\n",
      "[1218]\tvalid_0's multi_logloss: 1.41573\n",
      "[1219]\tvalid_0's multi_logloss: 1.41577\n",
      "[1220]\tvalid_0's multi_logloss: 1.41579\n",
      "[1221]\tvalid_0's multi_logloss: 1.41584\n",
      "[1222]\tvalid_0's multi_logloss: 1.41583\n",
      "[1223]\tvalid_0's multi_logloss: 1.41583\n",
      "[1224]\tvalid_0's multi_logloss: 1.41578\n",
      "[1225]\tvalid_0's multi_logloss: 1.41573\n",
      "[1226]\tvalid_0's multi_logloss: 1.41569\n",
      "[1227]\tvalid_0's multi_logloss: 1.41564\n",
      "[1228]\tvalid_0's multi_logloss: 1.41563\n",
      "[1229]\tvalid_0's multi_logloss: 1.41562\n",
      "[1230]\tvalid_0's multi_logloss: 1.41562\n",
      "[1231]\tvalid_0's multi_logloss: 1.41561\n",
      "[1232]\tvalid_0's multi_logloss: 1.41558\n",
      "[1233]\tvalid_0's multi_logloss: 1.41557\n",
      "[1234]\tvalid_0's multi_logloss: 1.41552\n",
      "[1235]\tvalid_0's multi_logloss: 1.41549\n",
      "[1236]\tvalid_0's multi_logloss: 1.41545\n",
      "[1237]\tvalid_0's multi_logloss: 1.41545\n",
      "[1238]\tvalid_0's multi_logloss: 1.41537\n",
      "[1239]\tvalid_0's multi_logloss: 1.41539\n",
      "[1240]\tvalid_0's multi_logloss: 1.4154\n",
      "[1241]\tvalid_0's multi_logloss: 1.41537\n",
      "[1242]\tvalid_0's multi_logloss: 1.41535\n",
      "[1243]\tvalid_0's multi_logloss: 1.41535\n",
      "[1244]\tvalid_0's multi_logloss: 1.41533\n",
      "[1245]\tvalid_0's multi_logloss: 1.41535\n",
      "[1246]\tvalid_0's multi_logloss: 1.41533\n",
      "[1247]\tvalid_0's multi_logloss: 1.41525\n",
      "[1248]\tvalid_0's multi_logloss: 1.41525\n",
      "[1249]\tvalid_0's multi_logloss: 1.41527\n",
      "[1250]\tvalid_0's multi_logloss: 1.41526\n",
      "[1251]\tvalid_0's multi_logloss: 1.41525\n",
      "[1252]\tvalid_0's multi_logloss: 1.41517\n",
      "[1253]\tvalid_0's multi_logloss: 1.41511\n",
      "[1254]\tvalid_0's multi_logloss: 1.41507\n",
      "[1255]\tvalid_0's multi_logloss: 1.41503\n",
      "[1256]\tvalid_0's multi_logloss: 1.41506\n",
      "[1257]\tvalid_0's multi_logloss: 1.41507\n",
      "[1258]\tvalid_0's multi_logloss: 1.41503\n",
      "[1259]\tvalid_0's multi_logloss: 1.41506\n",
      "[1260]\tvalid_0's multi_logloss: 1.41506\n",
      "[1261]\tvalid_0's multi_logloss: 1.41508\n",
      "[1262]\tvalid_0's multi_logloss: 1.41513\n",
      "[1263]\tvalid_0's multi_logloss: 1.4151\n",
      "[1264]\tvalid_0's multi_logloss: 1.41509\n",
      "[1265]\tvalid_0's multi_logloss: 1.41509\n",
      "[1266]\tvalid_0's multi_logloss: 1.41502\n",
      "[1267]\tvalid_0's multi_logloss: 1.41508\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1268]\tvalid_0's multi_logloss: 1.41508\n",
      "[1269]\tvalid_0's multi_logloss: 1.4151\n",
      "[1270]\tvalid_0's multi_logloss: 1.4151\n",
      "[1271]\tvalid_0's multi_logloss: 1.41505\n",
      "[1272]\tvalid_0's multi_logloss: 1.41502\n",
      "[1273]\tvalid_0's multi_logloss: 1.41504\n",
      "[1274]\tvalid_0's multi_logloss: 1.41504\n",
      "[1275]\tvalid_0's multi_logloss: 1.41498\n",
      "[1276]\tvalid_0's multi_logloss: 1.41495\n",
      "[1277]\tvalid_0's multi_logloss: 1.41492\n",
      "[1278]\tvalid_0's multi_logloss: 1.41491\n",
      "[1279]\tvalid_0's multi_logloss: 1.41492\n",
      "[1280]\tvalid_0's multi_logloss: 1.4149\n",
      "[1281]\tvalid_0's multi_logloss: 1.41491\n",
      "[1282]\tvalid_0's multi_logloss: 1.41489\n",
      "[1283]\tvalid_0's multi_logloss: 1.4149\n",
      "[1284]\tvalid_0's multi_logloss: 1.41487\n",
      "[1285]\tvalid_0's multi_logloss: 1.41486\n",
      "[1286]\tvalid_0's multi_logloss: 1.41485\n",
      "[1287]\tvalid_0's multi_logloss: 1.41482\n",
      "[1288]\tvalid_0's multi_logloss: 1.41479\n",
      "[1289]\tvalid_0's multi_logloss: 1.41481\n",
      "[1290]\tvalid_0's multi_logloss: 1.41478\n",
      "[1291]\tvalid_0's multi_logloss: 1.41471\n",
      "[1292]\tvalid_0's multi_logloss: 1.41472\n",
      "[1293]\tvalid_0's multi_logloss: 1.41472\n",
      "[1294]\tvalid_0's multi_logloss: 1.41471\n",
      "[1295]\tvalid_0's multi_logloss: 1.41469\n",
      "[1296]\tvalid_0's multi_logloss: 1.41467\n",
      "[1297]\tvalid_0's multi_logloss: 1.41465\n",
      "[1298]\tvalid_0's multi_logloss: 1.41463\n",
      "[1299]\tvalid_0's multi_logloss: 1.41462\n",
      "[1300]\tvalid_0's multi_logloss: 1.41463\n",
      "[1301]\tvalid_0's multi_logloss: 1.41466\n",
      "[1302]\tvalid_0's multi_logloss: 1.41461\n",
      "[1303]\tvalid_0's multi_logloss: 1.41461\n",
      "[1304]\tvalid_0's multi_logloss: 1.4146\n",
      "[1305]\tvalid_0's multi_logloss: 1.41459\n",
      "[1306]\tvalid_0's multi_logloss: 1.41459\n",
      "[1307]\tvalid_0's multi_logloss: 1.41458\n",
      "[1308]\tvalid_0's multi_logloss: 1.41454\n",
      "[1309]\tvalid_0's multi_logloss: 1.41458\n",
      "[1310]\tvalid_0's multi_logloss: 1.41458\n",
      "[1311]\tvalid_0's multi_logloss: 1.41458\n",
      "[1312]\tvalid_0's multi_logloss: 1.41455\n",
      "[1313]\tvalid_0's multi_logloss: 1.41452\n",
      "[1314]\tvalid_0's multi_logloss: 1.41447\n",
      "[1315]\tvalid_0's multi_logloss: 1.4145\n",
      "[1316]\tvalid_0's multi_logloss: 1.41451\n",
      "[1317]\tvalid_0's multi_logloss: 1.41452\n",
      "[1318]\tvalid_0's multi_logloss: 1.41449\n",
      "[1319]\tvalid_0's multi_logloss: 1.41442\n",
      "[1320]\tvalid_0's multi_logloss: 1.41442\n",
      "[1321]\tvalid_0's multi_logloss: 1.41444\n",
      "[1322]\tvalid_0's multi_logloss: 1.41439\n",
      "[1323]\tvalid_0's multi_logloss: 1.41441\n",
      "[1324]\tvalid_0's multi_logloss: 1.4144\n",
      "[1325]\tvalid_0's multi_logloss: 1.41442\n",
      "[1326]\tvalid_0's multi_logloss: 1.41442\n",
      "[1327]\tvalid_0's multi_logloss: 1.4144\n",
      "[1328]\tvalid_0's multi_logloss: 1.4144\n",
      "[1329]\tvalid_0's multi_logloss: 1.41437\n",
      "[1330]\tvalid_0's multi_logloss: 1.41433\n",
      "[1331]\tvalid_0's multi_logloss: 1.41436\n",
      "[1332]\tvalid_0's multi_logloss: 1.41436\n",
      "[1333]\tvalid_0's multi_logloss: 1.41434\n",
      "[1334]\tvalid_0's multi_logloss: 1.41433\n",
      "[1335]\tvalid_0's multi_logloss: 1.41433\n",
      "[1336]\tvalid_0's multi_logloss: 1.41436\n",
      "[1337]\tvalid_0's multi_logloss: 1.41433\n",
      "[1338]\tvalid_0's multi_logloss: 1.4143\n",
      "[1339]\tvalid_0's multi_logloss: 1.4143\n",
      "[1340]\tvalid_0's multi_logloss: 1.41427\n",
      "[1341]\tvalid_0's multi_logloss: 1.41424\n",
      "[1342]\tvalid_0's multi_logloss: 1.41423\n",
      "[1343]\tvalid_0's multi_logloss: 1.41421\n",
      "[1344]\tvalid_0's multi_logloss: 1.4142\n",
      "[1345]\tvalid_0's multi_logloss: 1.41419\n",
      "[1346]\tvalid_0's multi_logloss: 1.41421\n",
      "[1347]\tvalid_0's multi_logloss: 1.41422\n",
      "[1348]\tvalid_0's multi_logloss: 1.41418\n",
      "[1349]\tvalid_0's multi_logloss: 1.41416\n",
      "[1350]\tvalid_0's multi_logloss: 1.41419\n",
      "[1351]\tvalid_0's multi_logloss: 1.4142\n",
      "[1352]\tvalid_0's multi_logloss: 1.41421\n",
      "[1353]\tvalid_0's multi_logloss: 1.41415\n",
      "[1354]\tvalid_0's multi_logloss: 1.41415\n",
      "[1355]\tvalid_0's multi_logloss: 1.41417\n",
      "[1356]\tvalid_0's multi_logloss: 1.41416\n",
      "[1357]\tvalid_0's multi_logloss: 1.41416\n",
      "[1358]\tvalid_0's multi_logloss: 1.41415\n",
      "[1359]\tvalid_0's multi_logloss: 1.41417\n",
      "[1360]\tvalid_0's multi_logloss: 1.41414\n",
      "[1361]\tvalid_0's multi_logloss: 1.41415\n",
      "[1362]\tvalid_0's multi_logloss: 1.41417\n",
      "[1363]\tvalid_0's multi_logloss: 1.41418\n",
      "[1364]\tvalid_0's multi_logloss: 1.41419\n",
      "[1365]\tvalid_0's multi_logloss: 1.41416\n",
      "[1366]\tvalid_0's multi_logloss: 1.4141\n",
      "[1367]\tvalid_0's multi_logloss: 1.4141\n",
      "[1368]\tvalid_0's multi_logloss: 1.41407\n",
      "[1369]\tvalid_0's multi_logloss: 1.41405\n",
      "[1370]\tvalid_0's multi_logloss: 1.4141\n",
      "[1371]\tvalid_0's multi_logloss: 1.41411\n",
      "[1372]\tvalid_0's multi_logloss: 1.41413\n",
      "[1373]\tvalid_0's multi_logloss: 1.4141\n",
      "[1374]\tvalid_0's multi_logloss: 1.41403\n",
      "[1375]\tvalid_0's multi_logloss: 1.41398\n",
      "[1376]\tvalid_0's multi_logloss: 1.41401\n",
      "[1377]\tvalid_0's multi_logloss: 1.41405\n",
      "[1378]\tvalid_0's multi_logloss: 1.41406\n",
      "[1379]\tvalid_0's multi_logloss: 1.41402\n",
      "[1380]\tvalid_0's multi_logloss: 1.41395\n",
      "[1381]\tvalid_0's multi_logloss: 1.41395\n",
      "[1382]\tvalid_0's multi_logloss: 1.41397\n",
      "[1383]\tvalid_0's multi_logloss: 1.41395\n",
      "[1384]\tvalid_0's multi_logloss: 1.41392\n",
      "[1385]\tvalid_0's multi_logloss: 1.4139\n",
      "[1386]\tvalid_0's multi_logloss: 1.41389\n",
      "[1387]\tvalid_0's multi_logloss: 1.4139\n",
      "[1388]\tvalid_0's multi_logloss: 1.4139\n",
      "[1389]\tvalid_0's multi_logloss: 1.41387\n",
      "[1390]\tvalid_0's multi_logloss: 1.41384\n",
      "[1391]\tvalid_0's multi_logloss: 1.41383\n",
      "[1392]\tvalid_0's multi_logloss: 1.41382\n",
      "[1393]\tvalid_0's multi_logloss: 1.41379\n",
      "[1394]\tvalid_0's multi_logloss: 1.41378\n",
      "[1395]\tvalid_0's multi_logloss: 1.41379\n",
      "[1396]\tvalid_0's multi_logloss: 1.41379\n",
      "[1397]\tvalid_0's multi_logloss: 1.4138\n",
      "[1398]\tvalid_0's multi_logloss: 1.4138\n",
      "[1399]\tvalid_0's multi_logloss: 1.41379\n",
      "[1400]\tvalid_0's multi_logloss: 1.4138\n",
      "[1401]\tvalid_0's multi_logloss: 1.4138\n",
      "[1402]\tvalid_0's multi_logloss: 1.4138\n",
      "[1403]\tvalid_0's multi_logloss: 1.41378\n",
      "[1404]\tvalid_0's multi_logloss: 1.41379\n",
      "[1405]\tvalid_0's multi_logloss: 1.41379\n",
      "[1406]\tvalid_0's multi_logloss: 1.41375\n",
      "[1407]\tvalid_0's multi_logloss: 1.41377\n",
      "[1408]\tvalid_0's multi_logloss: 1.41378\n",
      "[1409]\tvalid_0's multi_logloss: 1.41377\n",
      "[1410]\tvalid_0's multi_logloss: 1.41372\n",
      "[1411]\tvalid_0's multi_logloss: 1.41368\n",
      "[1412]\tvalid_0's multi_logloss: 1.41364\n",
      "[1413]\tvalid_0's multi_logloss: 1.4136\n",
      "[1414]\tvalid_0's multi_logloss: 1.41358\n",
      "[1415]\tvalid_0's multi_logloss: 1.41357\n",
      "[1416]\tvalid_0's multi_logloss: 1.41355\n",
      "[1417]\tvalid_0's multi_logloss: 1.41358\n",
      "[1418]\tvalid_0's multi_logloss: 1.41356\n",
      "[1419]\tvalid_0's multi_logloss: 1.41356\n",
      "[1420]\tvalid_0's multi_logloss: 1.41353\n",
      "[1421]\tvalid_0's multi_logloss: 1.41351\n",
      "[1422]\tvalid_0's multi_logloss: 1.41357\n",
      "[1423]\tvalid_0's multi_logloss: 1.41356\n",
      "[1424]\tvalid_0's multi_logloss: 1.41355\n",
      "[1425]\tvalid_0's multi_logloss: 1.41354\n",
      "[1426]\tvalid_0's multi_logloss: 1.41349\n",
      "[1427]\tvalid_0's multi_logloss: 1.41349\n",
      "[1428]\tvalid_0's multi_logloss: 1.41353\n",
      "[1429]\tvalid_0's multi_logloss: 1.41352\n",
      "[1430]\tvalid_0's multi_logloss: 1.41351\n",
      "[1431]\tvalid_0's multi_logloss: 1.41345\n",
      "[1432]\tvalid_0's multi_logloss: 1.41345\n",
      "[1433]\tvalid_0's multi_logloss: 1.41344\n",
      "[1434]\tvalid_0's multi_logloss: 1.41345\n",
      "[1435]\tvalid_0's multi_logloss: 1.41339\n",
      "[1436]\tvalid_0's multi_logloss: 1.41338\n",
      "[1437]\tvalid_0's multi_logloss: 1.41338\n",
      "[1438]\tvalid_0's multi_logloss: 1.41344\n",
      "[1439]\tvalid_0's multi_logloss: 1.41342\n",
      "[1440]\tvalid_0's multi_logloss: 1.41342\n",
      "[1441]\tvalid_0's multi_logloss: 1.41338\n",
      "[1442]\tvalid_0's multi_logloss: 1.4134\n",
      "[1443]\tvalid_0's multi_logloss: 1.41342\n",
      "[1444]\tvalid_0's multi_logloss: 1.41343\n",
      "[1445]\tvalid_0's multi_logloss: 1.41343\n",
      "[1446]\tvalid_0's multi_logloss: 1.4134\n",
      "[1447]\tvalid_0's multi_logloss: 1.41342\n",
      "[1448]\tvalid_0's multi_logloss: 1.41337\n",
      "[1449]\tvalid_0's multi_logloss: 1.41335\n",
      "[1450]\tvalid_0's multi_logloss: 1.41336\n",
      "[1451]\tvalid_0's multi_logloss: 1.41335\n",
      "[1452]\tvalid_0's multi_logloss: 1.41336\n",
      "[1453]\tvalid_0's multi_logloss: 1.41335\n",
      "[1454]\tvalid_0's multi_logloss: 1.41331\n",
      "[1455]\tvalid_0's multi_logloss: 1.4133\n",
      "[1456]\tvalid_0's multi_logloss: 1.4133\n",
      "[1457]\tvalid_0's multi_logloss: 1.41327\n",
      "[1458]\tvalid_0's multi_logloss: 1.41329\n",
      "[1459]\tvalid_0's multi_logloss: 1.41325\n",
      "[1460]\tvalid_0's multi_logloss: 1.41324\n",
      "[1461]\tvalid_0's multi_logloss: 1.41322\n",
      "[1462]\tvalid_0's multi_logloss: 1.41321\n",
      "[1463]\tvalid_0's multi_logloss: 1.41323\n",
      "[1464]\tvalid_0's multi_logloss: 1.41324\n",
      "[1465]\tvalid_0's multi_logloss: 1.41327\n",
      "[1466]\tvalid_0's multi_logloss: 1.41327\n",
      "[1467]\tvalid_0's multi_logloss: 1.41326\n",
      "[1468]\tvalid_0's multi_logloss: 1.41323\n",
      "[1469]\tvalid_0's multi_logloss: 1.4132\n",
      "[1470]\tvalid_0's multi_logloss: 1.41318\n",
      "[1471]\tvalid_0's multi_logloss: 1.41317\n",
      "[1472]\tvalid_0's multi_logloss: 1.41318\n",
      "[1473]\tvalid_0's multi_logloss: 1.41317\n",
      "[1474]\tvalid_0's multi_logloss: 1.41321\n",
      "[1475]\tvalid_0's multi_logloss: 1.41321\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1476]\tvalid_0's multi_logloss: 1.4132\n",
      "[1477]\tvalid_0's multi_logloss: 1.41315\n",
      "[1478]\tvalid_0's multi_logloss: 1.41312\n",
      "[1479]\tvalid_0's multi_logloss: 1.41309\n",
      "[1480]\tvalid_0's multi_logloss: 1.41308\n",
      "[1481]\tvalid_0's multi_logloss: 1.41312\n",
      "[1482]\tvalid_0's multi_logloss: 1.41306\n",
      "[1483]\tvalid_0's multi_logloss: 1.41303\n",
      "[1484]\tvalid_0's multi_logloss: 1.41298\n",
      "[1485]\tvalid_0's multi_logloss: 1.41296\n",
      "[1486]\tvalid_0's multi_logloss: 1.41299\n",
      "[1487]\tvalid_0's multi_logloss: 1.41294\n",
      "[1488]\tvalid_0's multi_logloss: 1.41296\n",
      "[1489]\tvalid_0's multi_logloss: 1.41292\n",
      "[1490]\tvalid_0's multi_logloss: 1.41294\n",
      "[1491]\tvalid_0's multi_logloss: 1.41295\n",
      "[1492]\tvalid_0's multi_logloss: 1.41291\n",
      "[1493]\tvalid_0's multi_logloss: 1.41293\n",
      "[1494]\tvalid_0's multi_logloss: 1.41293\n",
      "[1495]\tvalid_0's multi_logloss: 1.41291\n",
      "[1496]\tvalid_0's multi_logloss: 1.41292\n",
      "[1497]\tvalid_0's multi_logloss: 1.4129\n",
      "[1498]\tvalid_0's multi_logloss: 1.41289\n",
      "[1499]\tvalid_0's multi_logloss: 1.41291\n",
      "[1500]\tvalid_0's multi_logloss: 1.41296\n",
      "[1501]\tvalid_0's multi_logloss: 1.41294\n",
      "[1502]\tvalid_0's multi_logloss: 1.41292\n",
      "[1503]\tvalid_0's multi_logloss: 1.41298\n",
      "[1504]\tvalid_0's multi_logloss: 1.41299\n",
      "[1505]\tvalid_0's multi_logloss: 1.41302\n",
      "[1506]\tvalid_0's multi_logloss: 1.41303\n",
      "[1507]\tvalid_0's multi_logloss: 1.41305\n",
      "[1508]\tvalid_0's multi_logloss: 1.41298\n",
      "[1509]\tvalid_0's multi_logloss: 1.41295\n",
      "[1510]\tvalid_0's multi_logloss: 1.41295\n",
      "[1511]\tvalid_0's multi_logloss: 1.41297\n",
      "[1512]\tvalid_0's multi_logloss: 1.41293\n",
      "[1513]\tvalid_0's multi_logloss: 1.41292\n",
      "[1514]\tvalid_0's multi_logloss: 1.4129\n",
      "[1515]\tvalid_0's multi_logloss: 1.41291\n",
      "[1516]\tvalid_0's multi_logloss: 1.41292\n",
      "[1517]\tvalid_0's multi_logloss: 1.41293\n",
      "[1518]\tvalid_0's multi_logloss: 1.41293\n",
      "Early stopping, best iteration is:\n",
      "[1498]\tvalid_0's multi_logloss: 1.41289\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\joel\\1.2017-3a\\1.oma\\deeplearning\\deep-learning-course\\dl-venv\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train score: 0.818 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\joel\\1.2017-3a\\1.oma\\deeplearning\\deep-learning-course\\dl-venv\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid score: 0.381\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\joel\\1.2017-3a\\1.oma\\deeplearning\\deep-learning-course\\dl-venv\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    }
   ],
=======
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
>>>>>>> c4e9ceadf4ee5e2ccdbf1233b43616164c979c88
   "source": [
    "# BONUS!\n",
    "# 5 - Try to improve performance with another classifier\n",
    "#     Attach the output file \"XXX_bov_y_test_sst.txt\" to your deliverable (where XXX = the name of the classifier)\n",
    "\n",
<<<<<<< HEAD
    "import lightgbm as lgb\n",
    "\n",
    "gboost_model = lgb.LGBMClassifier(objective='multiclass', n_estimators=2000, reg_lambda=5000)\n",
    "gboost_model.fit(normalized_encoded_train_data_weighted, labels_train,\n",
    "                   eval_set=[(normalized_encoded_valid_data_weighted, labels_valid)], early_stopping_rounds=20)\n",
    "\n",
    "print('Train score: %.3f ' % gboost_model.score(normalized_encoded_train_data_weighted, labels_train))\n",
    "print('Valid score: %.3f' % gboost_model.score(normalized_encoded_valid_data_weighted, labels_valid))\n",
    "\n",
    "prediction_test = gboost_model.predict(normalized_encoded_test_data_weighted)\n",
    "with open('./gboost_bov_y_test_sst.txt', 'w') as f1:\n",
    "    for k in prediction_test:\n",
    "        f1.write(str(k) + os.linesep)\n"
=======
    "# TYPE CODE HERE"
>>>>>>> c4e9ceadf4ee5e2ccdbf1233b43616164c979c88
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4) Sentence classification with LSTMs in Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 - Preprocessing"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 16,
=======
   "execution_count": 10,
>>>>>>> c4e9ceadf4ee5e2ccdbf1233b43616164c979c88
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
      "d:\\joel\\1.2017-3a\\1.oma\\deeplearning\\deep-learning-course\\dl-venv\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
=======
      "Using Theano backend.\n"
>>>>>>> c4e9ceadf4ee5e2ccdbf1233b43616164c979c88
     ]
    }
   ],
   "source": [
    "import keras"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 - Load train/dev/test sets of SST\n",
    "PATH_TO_DATA = \"data/\"\n",
    "\n",
    "# TYPE CODE HERE\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "n_classes=5\n",
    "\n",
    "words=[]\n",
    "sentences_train = []\n",
    "Y_train = []\n",
    "file = open(os.path.join(PATH_TO_DATA, 'SST/stsa.fine.train'), \"r\")\n",
    "for row in file:\n",
    "    sentences_train.append(row[1:])\n",
    "    Y_train.append(row[0])\n",
    "    for w in row.split()[1:]:\n",
    "        words.append(w)\n",
    "\n",
    "sentences_valid = []\n",
    "Y_valid = []\n",
    "file = open(os.path.join(PATH_TO_DATA, 'SST/stsa.fine.dev'), \"r\")\n",
    "for row in file:\n",
    "    sentences_valid.append(row[1:])\n",
    "    Y_valid.append(row[0])\n",
    "    for w in row.split()[1:]:\n",
    "        words.append(w)\n",
    "\n",
    "sentences_test = []\n",
    "file = open(os.path.join(PATH_TO_DATA, 'SST/stsa.fine.test.X'), \"r\")\n",
    "for row in file:\n",
    "    sentences_test.append(row)\n",
    "    for w in row.split():\n",
    "        words.append(w)\n",
    "        \n",
    "Y_valid_temp = [int(l) for l in Y_valid]\n",
    "Y_train_temp = [int(l) for l in Y_train]\n",
    "\n",
    "Y_train = to_categorical(Y_train_temp, num_classes=n_classes)\n",
    "Y_valid = to_categorical(Y_valid_temp, num_classes=n_classes)\n",
    "\n",
    "vocab_size = len(words)"
=======
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 1 - Load train/dev/test sets of SST\n",
    "PATH_TO_DATA = \"../../data/\"\n",
    "\n",
    "# TYPE CODE HERE\n"
>>>>>>> c4e9ceadf4ee5e2ccdbf1233b43616164c979c88
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 18,
   "metadata": {},
=======
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
>>>>>>> c4e9ceadf4ee5e2ccdbf1233b43616164c979c88
   "outputs": [],
   "source": [
    "# 2 - Transform text to integers using keras.preprocessing.text.one_hot function\n",
    "#     https://keras.io/preprocessing/text/\n",
    "\n",
<<<<<<< HEAD
    "# TYPE CODE HERE\n",
    "from keras.preprocessing.text import one_hot\n",
    "\n",
    "X_train = [one_hot(sentence, vocab_size) for sentence in sentences_train]\n",
    "X_valid = [one_hot(sentence, vocab_size) for sentence in sentences_valid]\n",
    "X_test = [one_hot(sentence, vocab_size) for sentence in sentences_test]"
=======
    "# TYPE CODE HERE\n"
>>>>>>> c4e9ceadf4ee5e2ccdbf1233b43616164c979c88
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Padding input data**\n",
    "\n",
    "Models in Keras (and elsewhere) take batches of sentences of the same length as input. It is because Deep Learning framework have been designed to handle well Tensors, which are particularly suited for fast computation on the GPU.\n",
    "\n",
    "Since sentences have different sizes, we \"pad\" them. That is, we add dummy \"padding\" tokens so that they all have the same length.\n",
    "\n",
    "The input to a Keras model thus has this size : (batchsize, maxseqlen) where maxseqlen is the maximum length of a sentence in the batch."
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 19,
   "metadata": {},
=======
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
>>>>>>> c4e9ceadf4ee5e2ccdbf1233b43616164c979c88
   "outputs": [],
   "source": [
    "# 3 - Pad your sequences using keras.preprocessing.sequence.pad_sequences\n",
    "#     https://keras.io/preprocessing/sequence/\n",
    "\n",
<<<<<<< HEAD
    "# TYPE CODE HERE\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "maxseqlen = 50\n",
    "\n",
    "X_train = pad_sequences(X_train, maxseqlen)\n",
    "X_valid = pad_sequences(X_valid, maxseqlen)\n",
    "X_test = pad_sequences(X_test, maxseqlen)"
=======
    "# TYPE CODE HERE\n"
>>>>>>> c4e9ceadf4ee5e2ccdbf1233b43616164c979c88
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 - Design and train your model"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\joel\\1.2017-3a\\1.oma\\deeplearning\\deep-learning-course\\dl-venv\\lib\\site-packages\\ipykernel_launcher.py:20: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(64, dropout=0.4, recurrent_dropout=0.4)`\n"
     ]
    }
   ],
=======
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
>>>>>>> c4e9ceadf4ee5e2ccdbf1233b43616164c979c88
   "source": [
    "# 4 - Design your encoder + classifier using keras.layers\n",
    "#     In Keras, Torch and other deep learning framework, we create a \"container\" which is the Sequential() module.\n",
    "#     Then we add components to this contained : the lookuptable, the LSTM, the classifier etc.\n",
    "#     All of these components are contained in the Sequential() and are trained together.\n",
    "\n",
    "\n",
    "# ADAPT CODE BELOW\n",
    "\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, LSTM, Dense, Activation\n",
    "\n",
    "embed_dim  = 32  # word embedding dimension\n",
    "nhid       = 64  # number of hidden units in the LSTM\n",
<<<<<<< HEAD
    "vocab_size = len(words)  # size of the vocabulary\n",
=======
    "vocab_size = 0  # size of the vocabulary\n",
>>>>>>> c4e9ceadf4ee5e2ccdbf1233b43616164c979c88
    "n_classes  = 5\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, embed_dim))\n",
<<<<<<< HEAD
    "model.add(LSTM(nhid, dropout_W=0.4, dropout_U=0.4))\n",
=======
    "model.add(LSTM(nhid, dropout_W=0.2, dropout_U=0.2))\n",
>>>>>>> c4e9ceadf4ee5e2ccdbf1233b43616164c979c88
    "model.add(Dense(n_classes, activation='sigmoid'))\n"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, None, 32)          7271840   \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 64)                24832     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 5)                 325       \n",
      "=================================================================\n",
      "Total params: 7,296,997\n",
      "Trainable params: 7,296,997\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# 5 - Define your loss/optimizer/metrics\n",
    "# MODIFY CODE BELOW\n",
    "\n",
    "loss_classif     =  'categorical_crossentropy'# find the right loss for multi-class classification\n",
    "optimizer        =  'adam' # find the right optimizer\n",
=======
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5 - Define your loss/optimizer/metrics\n",
    "\n",
    "# MODIFY CODE BELOW\n",
    "\n",
    "loss_classif     =  '' # find the right loss for multi-class classification\n",
    "optimizer        =  '' # find the right optimizer\n",
>>>>>>> c4e9ceadf4ee5e2ccdbf1233b43616164c979c88
    "metrics_classif  =  ['accuracy']\n",
    "\n",
    "# Observe how easy (but blackboxed) this is in Keras\n",
    "model.compile(loss=loss_classif,\n",
    "              optimizer=optimizer,\n",
    "              metrics=metrics_classif)\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8544 samples, validate on 1101 samples\n",
      "Epoch 1/6\n",
      "8544/8544 [==============================] - 19s 2ms/step - loss: 1.5783 - acc: 0.2725 - val_loss: 1.5723 - val_acc: 0.2534\n",
      "Epoch 2/6\n",
      "8544/8544 [==============================] - 18s 2ms/step - loss: 1.5379 - acc: 0.3076 - val_loss: 1.4794 - val_acc: 0.3660\n",
      "Epoch 3/6\n",
      "8544/8544 [==============================] - 18s 2ms/step - loss: 1.3540 - acc: 0.4228 - val_loss: 1.3908 - val_acc: 0.3951\n",
      "Epoch 4/6\n",
      "8544/8544 [==============================] - 17s 2ms/step - loss: 1.1397 - acc: 0.4913 - val_loss: 1.3792 - val_acc: 0.4033\n",
      "Epoch 5/6\n",
      "8544/8544 [==============================] - 17s 2ms/step - loss: 0.9534 - acc: 0.5939 - val_loss: 1.5225 - val_acc: 0.4033\n",
      "Epoch 6/6\n",
      "8544/8544 [==============================] - 18s 2ms/step - loss: 0.7857 - acc: 0.6935 - val_loss: 1.6034 - val_acc: 0.3760\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x13138ec1b38>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
=======
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
>>>>>>> c4e9ceadf4ee5e2ccdbf1233b43616164c979c88
   "source": [
    "# 6 - Train your model and find the best hyperparameters for your dev set\n",
    "#     you will be evaluated on the quality of your predictions on the test set\n",
    "\n",
    "# ADAPT CODE BELOW\n",
<<<<<<< HEAD
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "bs = 64\n",
    "n_epochs = 6\n",
    "\n",
    "history = model.fit(X_train, Y_train, batch_size=bs, epochs=n_epochs, validation_data=(X_valid, Y_valid))\n",
    "\n",
    "#Model Visualisation\n",
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title('model accuracy')\n",
    "axes = plt.gca()\n",
    "axes.set_ylim([0,1])\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
=======
    "bs = 64\n",
    "n_epochs = 6\n",
    "\n",
    "history = model.fit(x_train, y_train, batch_size=bs, nb_epoch=n_epochs, validation_data=(x_val, y_val))"
>>>>>>> c4e9ceadf4ee5e2ccdbf1233b43616164c979c88
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 23,
   "metadata": {},
=======
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
>>>>>>> c4e9ceadf4ee5e2ccdbf1233b43616164c979c88
   "outputs": [],
   "source": [
    "# 7 - Generate your predictions on the test set using model.predict(x_test)\n",
    "#     https://keras.io/models/model/\n",
    "#     Log your predictions in a file (one line = one integer: 0,1,2,3,4)\n",
    "#     Attach the output file \"logreg_lstm_y_test_sst.txt\" to your deliverable.\n",
    "\n",
<<<<<<< HEAD
    "# TYPE CODE HERE\n",
    "pred_test = np.argmax(model.predict(X_test),axis=1)\n",
    "with open('./logreg_lstm_y_test_sst.txt', 'w') as f1:\n",
    "    for k in pred_test:\n",
    "        f1.write(str(k) + os.linesep)"
=======
    "# TYPE CODE HERE\n"
>>>>>>> c4e9ceadf4ee5e2ccdbf1233b43616164c979c88
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 -- innovate !"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train score: 0.518 \n",
      "Valid score: 0.295\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\joel\\1.2017-3a\\1.oma\\deeplearning\\deep-learning-course\\dl-venv\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "d:\\joel\\1.2017-3a\\1.oma\\deeplearning\\deep-learning-course\\dl-venv\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    }
   ],
=======
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
>>>>>>> c4e9ceadf4ee5e2ccdbf1233b43616164c979c88
   "source": [
    "# 8 - Open question: find a model that is better on your dev set\n",
    "#     (e.g: use a 1D ConvNet, use a better classifier, pretrain your lookup tables ..)\n",
    "#     you will get point if the results on the test set are better: be careful of not overfitting your dev set too much..\n",
    "#     Attach the output file \"XXX_XXX_y_test_sst.txt\" to your deliverable.\n",
    "\n",
<<<<<<< HEAD
    "# TYPE CODE HERE\n",
    "import lightgbm as lgb\n",
    "\n",
    "gboost_model = lgb.LGBMClassifier(objective='multiclass', n_estimators=2000, reg_lambda=10)\n",
    "gboost_model.fit(X_train, Y_train_temp, verbose=False,\n",
    "                   eval_set=[(X_valid, Y_valid_temp)], early_stopping_rounds=50)\n",
    "\n",
    "print('Train score: %.3f ' % gboost_model.score(X_train, Y_train_temp))\n",
    "print('Valid score: %.3f' % gboost_model.score(X_valid, Y_valid_temp))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_test = np.argmax(model.predict(X_test),axis=1)\n",
    "with open('./gboost_test_sst.txt', 'w') as f1:\n",
    "    for k in pred_test:\n",
    "        f1.write(str(k) + os.linesep)"
=======
    "# TYPE CODE HERE\n"
>>>>>>> c4e9ceadf4ee5e2ccdbf1233b43616164c979c88
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
